var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(n){const s=suggestions.classList.contains("d-none");if(s)return;const e=[...suggestions.querySelectorAll("a")];if(e.length===0)return;const t=e.indexOf(document.activeElement);if(n.key==="ArrowUp"){n.preventDefault();const s=t>0?t-1:0;e[s].focus()}else if(n.key==="ArrowDown"){n.preventDefault();const s=t+1<e.length?t+1:t;e[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/Help/docs/connect/",title:"Connect to CCB",description:"Connecting Doks.",content:""}).add({id:1,href:"/Help/docs/linux/",title:"Linux command line",description:"Guidance on following best practices while using the Linux command line.",content:""}).add({id:2,href:"/Help/docs/data-management/",title:"Data management",description:"Data management Doks.",content:""}).add({id:3,href:"/Help/docs/slurm/",title:"Slurm",description:"Slurm Doks.",content:""}).add({id:4,href:"/Help/docs/conda/",title:"Conda",description:"Conda Doks.",content:""}).add({id:5,href:"/Help/docs/python/",title:"Python",description:"Python Doks.",content:""}).add({id:6,href:"/Help/docs/r/",title:"R",description:"R Doks.",content:""}).add({id:7,href:"/Help/docs/pipelines/",title:"Pipelines",description:"Pipelines Doks.",content:""}).add({id:8,href:"/Help/docs/help/",title:"Help",description:"Help Doks.",content:""}).add({id:9,href:"/Help/docs/connect/first-steps/",title:"First steps",description:"First steps Doks.",content:""}).add({id:10,href:"/Help/docs/connect/first-steps/introduction/",title:"Introduction",description:"Welcome to the MRC WIMM CCB cluster. This page provides guidance on the use of this documentation.",content:"What this documentation is # This documentation describes the user-facing MRC WIMM CCB cluster infrastructure, how to connect to the CCB cluster, and best practices for using common software on the cluster.\nThis documentation focuses on introductory scenarios and simple use cases. For further reading, trusted sources of information are listed on the page Links to external resources which can be found in the main help section.\nWho this documentation is for # This documentation is mainly intended for people with a CCB account. Find out more on the page Apply for an account.\nHow to use this documentation # This documentation is organised as a chronological progression through the typical setup of new users of the MRC WIMM CCB services.\nWe encourage readers to follow instructions in this documentation in the order presented.\nðŸ‘‰  Following instructions out of order or skipping instructions may produce unexpected results.   Many parts of this documentation include screenshots for illustrative purposes. In those screenshots, user-specific directories and parameters are highlighted and blurred for privacy and to avoid confusion as much as possible. Furthermore, it is very likely that you notice slight differences between those screenshots and your own experience, especially with respect to new versions of software packages becoming available.\nIf you notice any significant difference between this documentation and your own experience, please Contact us to resolve the issue and update the documentation where necessary.\nGo further # Experienced users and those revisiting this website to refresh their knowledge are welcome to use the navigation menu on the left to explore specific topics or use the various cheatsheets available.\nAn auto-completing search bar is also available at the top of the page to search the website by keywords.\nContact us # Refer to the Help section for ways to contact us.\n"}).add({id:11,href:"/Help/docs/connect/first-steps/apply/",title:"Apply for an account",description:"Current Oxford academics, students and staff are entitled to hold a CCB account. However, they must explicitly apply to create it.",content:"Get started # Apply for an account on the CCB Account and Support â†’ page.\nðŸ‘‰  The MRC WIMM CCB services are available only to members of staff and eligible collaborators.   "}).add({id:12,href:"/Help/docs/connect/first-steps/vpn-setup/",title:"VPN setup",description:"The CCB cluster is accessible only from the University campus network. A VPN client is necessary to use the CCB services while travelling or working from home.",content:"Motivation # When your device is not directly connected to the University campus network (e.g., when you are working from home or travelling), the VPN Service allows you to access the University campus network, and thus the CCB cluster.\nThe web page Virtual private network (VPN) â†’ provide the latest information about requirements and instructions to set up recommended VPN clients. Most of the sections below refer to instructions on that page.\nSet up a VPN client # On the web page Virtual private network (VPN) â†’, expand the section \u0026lsquo;How to setup Cisco AnyConnect\u0026rsquo;.\nClick on the operating system for your device and follow the instructions.\nThe first time that you use the Cisco AnyConnect VPN client, the instructions will lead you to connect to the VPN using your username and password.\nSubsequently, your device may remember your username, but you will always be prompted for your password.\nThe Cisco AnyConnect VPN client # Once you have installed the Cisco AnyConnect VPN client and set up the University VPN profile during your first connection, the VPN client should display \u0026lsquo;University VPN\u0026rsquo; in the dropdown menu.\n  Connection # On the web page Virtual private network (VPN) â†’, expand the section \u0026lsquo;Connect using Cisco AnyConnect\u0026rsquo;.\nThe first time that you connect to the University VPN, you must type the University VPN address in the dropdown menu and click the button \u0026lsquo;Connect\u0026rsquo;.\nSubsequently, you can simply select \u0026lsquo;University VPN\u0026rsquo; in the dropdown menu and click the button \u0026lsquo;Connect\u0026rsquo;.\nThen, when prompted, type in your username and password. Then, click \u0026lsquo;OK\u0026rsquo;.\n  You are now connected to the University campus network, giving you access to the CCB cluster.\nDisconnection # On the web page Virtual private network (VPN) â†’, expand the section \u0026lsquo;Disconnect using Cisco AnyConnect\u0026rsquo;.\nYour device will use the University VPN service until you disconnect.\nWhile connected, you will see an icon in your computer\u0026rsquo;s system tray .\nClick on it to reveal the application\u0026rsquo;s menu.\n  In the menu, click \u0026lsquo;Disconnect\u0026rsquo;.\nYou are now disconnected from the University campus network and using only your own internet connection again.\nFurther reading # You can find useful informationÂ organised in various sections on the page Virtual private network (VPN) â†’.\nThe section \u0026lsquo;FAQ\u0026rsquo; details frequently asked questions and answers.\nThe section \u0026lsquo;Setup an alternate VPN client\u0026rsquo; details alternatives to the Cisco AnyConnect VPN client.\n"}).add({id:13,href:"/Help/docs/connect/first-steps/windows-setup/",title:"Microsoft Windows setup",description:"Microsoft Windows requires some additional setup to connect to the SSH cluster more conveniently.",content:"Install Git Bash for Windows # Motivation # Microsoft Windows does not have a built-in Terminal emulator (The MS Windows Command Prompt cmd.exe is a command-line interpreter that does not include many of the commands needed to connect to the CCB cluster).\nThe primary function of the Git Bash for Windows program is to provide a Bash emulation used to run Git from the command line. Conveniently, in its effort to offer that functionality, the Terminal emulator of Git Bash for Windows also includes all the commands that you will need to connect to the CCB cluster.\nDownload the installer file # In your web browser, navigate to https://git-scm.com/downloads.\nIn the \u0026lsquo;Downloads\u0026rsquo; section, click on \u0026lsquo;Windows\u0026rsquo;.\n  On the next page, click on the first link \u0026lsquo;Click here to download the latest 32-bit version of Git for Windows\u0026rsquo;.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nðŸ‘‰  The various screens of the installer programs are subject to change. As such, we provide guidance below for indicative purposes only.   Notably:\n Scroll to the bottom of the license. Click \u0026lsquo;Next\u0026rsquo;. Leave the destination location for the installation to the default value. Click \u0026lsquo;Next\u0026rsquo;. Leave the selected components to the default choices. Click \u0026lsquo;Next\u0026rsquo;. Leave the name of the Start Menu folder to be created to the default value. Click \u0026lsquo;Next\u0026rsquo;. Set the default editor for Git to a text editor program that is installed on your computer and that you are comfortable with. We recommend Atom, Notepad++ or Visual Studio Code as lightweight programs suitable for this purpose. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice letting Git decide the name for the initial branch in newly created repositories. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice adjusting your PATH environment. You want to be able to use Git from Git Bash, the Command Prompt and the Windows PowerShell as well as third-party software looking for Git in PATH (e.g., RStudio Desktop). Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using the bundled OpenSSH. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using the OpenSSL library. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using checkout Windows-style, commit Unix-style line endings. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using MinTTY. Click \u0026lsquo;Next\u0026rsquo;. Leave the default behaviour of git pull to the default value. Click \u0026lsquo;Next\u0026rsquo;. Leave the credential manager to the default value. Click \u0026lsquo;Next\u0026rsquo;. Leave the extra options to their default choices. Click \u0026lsquo;Next\u0026rsquo;. Leave the experimental options unselected. Click \u0026lsquo;Install\u0026rsquo;.  When the installer successfully completes, click \u0026lsquo;Finish\u0026rsquo;.\nYou may be presented with the Release Notes of the program, which you may choose to read before you close them.\nTest your installation # Once the installer has completed successfully:\n Open the Windows Start Menu. In the Windows Start Menu find and launch \u0026lsquo;Git Bash\u0026rsquo;. In the Git Bash for Windows Terminal emulator, type git and press the Return key.  This should display a help message listing the main subcommands of the Git program.\n"}).add({id:14,href:"/Help/docs/connect/first-steps/ssh-first-connection/",title:"First connection over SSH",description:"Users connecting to the CCB cluster for the first time must use the username and password send to them by email when their account was created.",content:"What is SSH? # SSH stands for Secure Shell.\nThis refers to an encrypted network protocol for establishing a secure connection to a remote server. The connection is materialised as a remote command line login to a Linux system.\nAt its simplest, the protocol uses a username and password to authenticate a user on the remote system.\nIt can be used to log into the CCB cluster.\nChoose a login node # The CCB cluster has two login nodes that users can log into using SSH:\n cbrglogin1.molbiol.ox.ac.uk cbrglogin3.molbiol.ox.ac.uk  Here is some guidance to choose a login node:\n Login nodes with more active users are less responsive. If you notice a drop in performance for simple commands (e.g., ls), consider logging out and connecting to a different login node. The login node cbrglogin3.molbiol.ox.ac.uk has more memory available (1TB), relative to only 250G on the other login nodes. If you are testing commands that require larger amounts of memory, consider connecting to that node.  Log in with your username and password # When your account is created on the CCB cluster, you will be sent an email including your unique username on the CCB cluster and a randomly generated password.\nTo log into a login node, open a Terminal on your personal computer and use the ssh command, combining your username, the @ symbol, and the name of the login node that you wish to log into. Conceptually, the command that you type in the Terminal should be structured as follows:\nssh \u0026lt;username\u0026gt;@\u0026lt;remote\u0026gt;  Replace \u0026lt;username\u0026gt; by your own username, and \u0026lt;remote\u0026gt; by one of the login nodes (see Login nodes section above). When you have typed the command above, press the Return key to execute it.\n  If this is indeed your very first connection, you may be prompted to verify the authenticity of the host. If this happens, type yes and press the Return key to continue connecting.\nWhen prompted for your password, type it and press the Return key.\nðŸ‘‰  For privacy and security reasons, the Terminal will not display your password as you type it. If you are having trouble typing your password correctly without seeing it, you can use a text editor to type it, then cut and paste it into the Terminal window when prompted.   You should now be logged in the MRC WIMM CCB high-performance computing (HPC) cluster.\nWelcome!\nLog out # To log out of the CCB cluster, you can use any of the following methods:\n Type exit and press the Return key. Type logout and press the Return key. Press the Control and D keys simultaneously.  "}).add({id:15,href:"/Help/docs/connect/first-steps/ssh-change-password/",title:"Change your SSH password",description:"Randomly generated passwords stored in emails are not secure. They must be replaced as soon as possible by new, secret, and memorable passwords.",content:"Motivation # The initial randomly generated password that you received by email is at risk of being stolen and misused if your email account were ever hacked. Separately, the randomness of the password makes it very difficult to remember and type manually.\nIt is highly recommended to change that initial password on your first connection to the CCB cluster.\nInstructions # First, log into the CCB cluster as demonstrated on the page First SSH connection.\nNext, type the command passwd and press the Return key. Then, follow the instructions returned by the command in the Terminal. In particular, when prompted, type (or copy-paste) your current password (the randomly generated password that you were given in the email), and then type your new desired password twice, pressing the Return key each time.\nYou should then see a message indicating that your password was updated successfully.\npasswd    ðŸ‘‰  For privacy and security reasons, the Terminal will not display passwords as you type (or copy-paste) them. If you are having trouble typing your password correctly without seeing it, you can use a text editor to type it, then cut and paste it into the Terminal window when prompted.   "}).add({id:16,href:"/Help/docs/connect/ssh-configuration/",title:"Configure SSH",description:"First steps Doks.",content:""}).add({id:17,href:"/Help/docs/connect/ssh-configuration/introduction/",title:"Benefits of configuring SSH",description:"SSH can be used more efficiently when configured.",content:"Motivation # In the section First connection over SSH, we describe how to connect to the CCB cluster, typing all the information necessary to establish the connection at the Terminal prompt.\nWhen connecting to CCB cluster on a regular basis \u0026ndash; often multiple times a day \u0026ndash;, typing that information becomes a tedious process that is also subject to typographical errors forcing users to start over the connection process.\nInstead, the SSH client can be configured to ease the process of connection to remote computers.\nIn the following pages, we describe steps commonly taken to configure an SSH client for connecting more rapidly and securely to the CCB cluster.\nPrerequisites # This section expects a minimum of familiarity with the Linux command line and commands that are described in later sections of this website.\nIn particular, you will be expected to:\n Execute Bash commands in a Linux environment. View and edit files in the Terminal. Optionally, Navigate directories.  Make sure that you are comfortable with the information listed above before attempting to configure your SSH client.\nYou may also find it helpful to keep those pages open while following the instructions to configure your SSH client.\nFinal advice # In doubt, ask a system adminstrator or an experienced colleague to guide you through the instructions in the following pages.\nThose instructions describe a one-time setup process.\nOnce you have gone through the process and tested that the new SSH configuration works, you can choose to establish new connections to the CCB cluster using either the newly configured shortcuts or the username and password as you did for your First connection over SSH.\n"}).add({id:18,href:"/Help/docs/connect/ssh-configuration/ssh-key-pair/",title:"Set up an SSH key pair",description:"SSH key pairs provide a more secure alternative to username and password for logging into remote systems.",content:"Motivation # An SSH key pair can be used as an additional layer of security to connect to your account on the CCB HPC cluster. Instead of typing your username and password, it is possible to log into your account on the CCB cluster using a pair of files; one file on your personal computer (the \u0026ldquo;private\u0026rdquo; key), one file on the CCB cluster (the \u0026ldquo;public\u0026rdquo; key). Together, those two files enable your personal computer and the cluster to identify themselves to each other.\n  Creating an SSH key pair # To create the SSH key pair, open a Terminal session on your personal computer (i.e., not logged into the CCB cluster), and type:\nssh-keygen -t ecdsa -b 521  In particular:\n The option -t ecdsa is important to specify the type of SSH key pair. The option -b 521 indicates the number of bits in the key; a minimum of 2048 is recommended, while larger values generally give additional strength to the key.  When prompted for a filename, do not type anything, and immediately press the Return key to accept the default filename and location.\nWhen prompted for a passphrase, CCB policy requires a 16 character passphrase. Private keys without a password are strictly prohibited.\nðŸ‘‰  For privacy and security reasons, the Terminal will not display your passphrase as you type it.   When prompted to confirm your passphrase, type the same passphrase and press the Return key.\nFinally, the Terminal should display a message indicating that the key pair was successfully created.\n  In particular:\n The private key is located at ~/.ssh/id_ecdsa. The public key is located at ~/.ssh/id_ecdsa.pub.  ðŸ‘‰  You can ignore the key's randomart.   Adding the public key on the CCB cluster # Finally, the public key \u0026ndash; generated and saved as a file on your own computer \u0026ndash; needs to be copied to your account on the CCB cluster, so that the CCB cluster can prove its identity to the client (see above, Motivation).\nConceptually, the command that you type in the Terminal on your personal computer should be structured as follows:\nssh-copy-id -i ~/.ssh/id_ecdsa \u0026lt;username\u0026gt;@\u0026lt;remote\u0026gt;  Replace \u0026lt;username\u0026gt; by your own username, and \u0026lt;remote\u0026gt; by one of the login nodes (see General information).\nIf the public key indeed does not exist on your CCB cluster account yet, you will be prompted for your password. Type it, and press the Return key.\nThis adds the public key to the file ~/.ssh/authorized_keys on your account on the CCB cluster.\n  Configuring the SSH client # Having set up the SSH key pair on your own computer and the CCB cluster, you still need to configure the SSH client so that it uses the private key whenever you invoke the ssh command, to prove your identity to the CCB cluster, and vice versa.\nOpen (or create) the file ~/.ssh/config on your personal computer. You may do so using any text editor of your choice, including graphical applications (e.g., Sublime Text) or Terminal text editors such as those listed in the section Editing files in the Terminal.\nIn that file, add the following lines:\nHost * IdentityFile ~/.ssh/id_ecdsa  Those lines configure the private key pair used for any remote host that you log into using the ssh command.\n The keyword Host creates a new set of parameters. The pattern * matches all hostnames, making those parameters global defaults unless overwritten in subsequent declarations. The field IdentityFile specifies the path to the private SSH key file that you created earlier on your personal computer.  Alternatively, if you happen to use different SSH key pairs to connect to different SSH servers, you may need to explicitly declare the name of the host next to the Host keyword, so that the following configuration settings only apply to that particular host.\nHost cbrglogin1.molbiol.ox.ac.uk IdentityFile ~/.ssh/id_ecdsa  More information about fields that may be configured is available on the page Advanced client configuration\nðŸ‘‰  We recommend keeping your SSH configuration file as simple and minimal as possible unless you identify the need for further configuration.   Log in using your SSH key pair # You can now log into the CCB cluster using the same ssh command as before. You will be prompted to type your SSH passphrase when the SSH key pair is used.\nssh \u0026lt;username\u0026gt;@\u0026lt;remote\u0026gt;  The screenshot below demonstrates the ssh command in action.\n  ðŸ‘‰  MacOS users may be prompted to type their optional passphrase only for the first connection. The macOS keychain is capable of remembering the passphrase and automatically using it for subsequent connections, which is why the the screenshot above does not prompt the user for the passphrase.   "}).add({id:19,href:"/Help/docs/connect/ssh-configuration/ssh-host/",title:"Configure an SSH host",description:"SSH hosts configures common parameters when connecting to certain remote hosts.",content:"Motivation # When regularly connecting to one or more remote hosts, it can be tedious to type or even remember all the parameters in the ssh command. Moreover, some parameters are common for multiple or all remote hosts, while other parameters are specific to individual hosts.\nIn this section, we describe how to configure parameters that are specific to different hosts.\nRecommended parameters shared across all remote hosts are described in the earlier page Set up an SSH key pair.\nConfigure a remote host # On your personal computer, open the file ~/.ssh/config. You may do so using any text editor of your choice, including graphical applications (e.g., Sublime Text) or Terminal text editors such as those listed in the section Editing files in the Terminal.\nAdd the following lines in the file, making sure to replace \u0026lt;username\u0026gt; by your own username:\nHost ccb1 Hostname cbrglogin1.molbiol.ox.ac.uk User \u0026lt;username\u0026gt; Host ccb3 Hostname cbrglogin3.molbiol.ox.ac.uk User \u0026lt;username\u0026gt;  Then, save and close the file.\nThose lines configure three remote hosts.\n The keyword Host creates a new host. The field Hostname configures the real hostname to log into. The field User specifies the username to log in as. The indentation of fields is optional. However, four spaces are commonly used for readability.  More information about fields that may be configured is available on the page ssh_config(5) - Linux man page â†’.\nConnect to a configured remote host # You should now be able to log into the CCB cluster using any of the following commands, to connect to the corresponding remote host:\nssh ccb1 ssh ccb3  Notice how you do not need to specify your username, the hostname, nor any of the parameters configured for all remote hosts.\nIn the screenshot below, notice the prompt changing. Initially, the ssh command is typed in the prompt of the personal computer. Once the connection is established, the prompt of the remote host appears (in this case, cbrglogin1).\n  "}).add({id:20,href:"/Help/docs/connect/ssh-configuration/client-advanced/",title:"Advanced client configuration",description:"SSH clients can be extensively configured, with caution.",content:"Motivation # SSH clients can be configured through the configuration file ~/.ssh/config.\nAn example of this is demonstrated in the earlier page Set up an SSH key pair.\nSome configuration settings are specific to individual operating systems. In the sections below, we provide a series of examples for the purpose of illustration and documentation.\nImportantly, all the configuration parameters presented in this page are all optional. They are presented here for the purpose of information and help. We do not recommend configuring any of those parameters in your own environment unless you identified the need for them.\nExamples # Windows # Host * IdentityFile ~/.ssh/id_ecdsa Port 22 Protocol 2 TCPKeepAlive yes ServerAliveInterval 300 ServerAliveCountMax 2 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes Compression yes XAuthLocation /opt/X11/bin/xauth  See the Cheatsheet section below for information about individual settings.\nmacOS # Host * IdentityFile ~/.ssh/id_ecdsa Port 22 Protocol 2 TCPKeepAlive yes ServerAliveInterval 300 ServerAliveCountMax 2 ForwardX11 yes ForwardX11Trusted yes ForwardAgent yes Compression yes XAuthLocation /opt/X11/bin/xauth # macOS only AddKeysToAgent yes UseKeychain yes PubkeyAuthentication yes  See the Cheatsheet section below for information about individual settings.\nCheatsheet # All operating systems #    Option Descrition     IdentityFile Path to the private SSH key file on your personal computer.   Port Port number to connect on the remote host (default: 22).   Protocol Protocol version of SSH to use.   TCPKeepAlive Whether TCP keepaline messages are regularly sent, which is useful to notice it the connection dies at any point.   ServerAliveInterval Timeout interval in seconds after which the client will request a response from the server if no data has been received recently.   ServerAliveCountMax Number of server alive messages which may be sent to the server without receiving any reply before ssh disconnects from the server.   ForwardX11 Whether X11 connections will be automatically redirected over the secure channel of the ssh connection.   ForwardX11Trusted Whether remote X11 clients are given full access to the original X11 display.   ForwardAgent Whether the connection to the authentication agent (if any) is forwarded to the remote machine.   Compression Whether to use compression.   XAuthLocation Full pathname of the xauth program.   PubkeyAuthentication Whether to try public key authentication using SSH keys.    macOS settings #    Option Descrition     AddKeysToAgent Whether keys should be automatically added to a running ssh-agent.   UseKeychain Whether passphrases are stored in the keychain.    More information about fields that may be configured for macOS is available on the page OpenBSD manual page server â†’.\nMore information on the field UseKeychain is available on the page Technical Note TN2449 â†’.\n"}).add({id:21,href:"/Help/docs/linux/essentials/",title:"Linux essentials",description:"",content:""}).add({id:22,href:"/Help/docs/linux/essentials/introduction/",title:"Introduction to Linux and Bash",description:"A brief introduction to Linux and Bash on the CCB cluster.",content:"What is Linux? # Linux is an operating system (OS), like Microsoft Windows (MS Windows) and macOS.\nHowever, Linux is open source and free of cost.\nLinux and macOS are different, but share a large number of commands as macOS is based on BSD (Berkeley Standard Distribution) which is similar to Linux but not Linux.\nMore technical differences include:\n File names are case-sensitive in Linux (and macOS) while they are not in MS Windows. File paths are separated by / (forward slash) on Linux (and macOS), while they are separated by \\ (back slash) on MS Windows.  The Linux file system # In the Linux filesystem, everything is a file.\nA directory is a file that contains the list of files and directories that it contains.\nAs a result, files organised in a tree structure. The first directory in that tree is called \u0026lsquo;root\u0026rsquo;, and it is represented by the symbol / (forward slash).\nThe schema below illustrates some of the core directories and files that make up a typical Linux filesystem.\n  ðŸ‘‰  On the CCB cluster, the directories and files that are essential to the operating system are protected; users do not have permission to access or edit those files.   What is Bash? # Bash (Bourne Again Shell) is the shell, or command language interpreter, for the GNU/Linux operating system. In other words, it is the programming language that is used to issue commands to the operating system from the Terminal application.\nHow do I use Bash? # If you have gone through the earlier section Connect to CCB, then congratulations: you have been using Bash already!\nFrom the moment you typed commands such as ssh and passwd in the Terminal application, you have been working in a Bash session, and issuing commands to the operating system from that Bash session.\nOnce logged into the CCB cluster, the Terminal application gives you access to the Linux shell \u0026ndash; i.e., the command line interface \u0026ndash; where you can type commands that are given to the operating system and executed in the current Bash environment.\nWhere do I go from here? # Once connected to the CCB cluster, you can use built-in Bash commands to:\n list and navigate directories in the filesystem create, edit, remove, open and execute files create and remove directories \u0026hellip;and much more!  In the following pages, this documentation introduces commonly used built-in Bash commands and options, and describing best practices on the CCB cluster.\n"}).add({id:23,href:"/Help/docs/linux/essentials/autocompletion/",title:"Autocompletion",description:"The autocompletion functionality saves time and mitigates the risk of typographical errors.",content:"What is autocompletion? # Autocompletion refer to the action of pressing the tabulation key after typing the first few characters of a command name or filepath, allowing the Terminal application to predict the rest of the word.\nIf the prediction unambiguously identifies a single match, the remaining characters are automatically inserted to complete the word. The user can then continue typing the rest of the command or filepath.\nIf the prediction identifies multiple matches, no autocompletion will take place. Instead, the tabulation key can be pressed a second time to reveal the list of ambiguous matches that were identified. If the list of matches identified is excessively large, the terminal may prompt for a y or n answer, whether to display all the possibilities. Either way, the prompt will be returned with the characters typed so far, encouraging the user to type more characters before attempting to autocomplete again.\n  The tabulation key # The tabulation key is located on the left side of most keyboards, above the Caps Lock key.\n  Why use autocompletion? # The first and most obvious benefit of autocompletion is the time saved automatically completing words at the touch of the tabulation key instead of manually typing every single character of every command.\nMeanwhile, a more subtle yet unbdoubtedly more impactful benefit of autocompletion is the mitigation of typographical mistakes introduced by human error during the manual typing of commands, leading to time saved not running commands that would have otherwise failed due to typographical errors.\nInstead, autocompletion acts as a dynamic sanity check that can be used to reveal whether the characters typed so far match the expected command or file path. In particular, autocompletion failing to identify any match is usually taken as a sign that a typographical error is present in the characters typed so far, prompting users to pause and proofread themselves.\nWhile the rest of this documentation does not explicitly refer to autocompletion, we encourage you to try it out and practice it regularly to build it into your habits as a Bash user on the CCB cluster.\n"}).add({id:24,href:"/Help/docs/linux/essentials/keyboard-shortcuts/",title:"Keyboard shortcuts",description:"Certain keys -- or combinations of keys -- trigger extremely useful functionality in a Bash terminal.",content:"Navigate the history of commands # The up and down arrow keys can be used to navigate the history of commands used earlier in a Terminal session (or in an earlier Terminal session altogether).\n  ðŸ‘‰  The left and right arrow keys have an entirely unrelated functionality; they navigate one character left or right within the command currently being typed at the prompt.   Jump to the start/end of a line # The Control and A keys can be pressed simultaneously to move the cursor instantly at the start of the current line.\nConversely, Control and E move the cursor to the end of the line.\nAbort the current command # The Control and C keys can be pressed simultaneously to interrupt a command that is currently running in the Terminal application.\nIn the example below, we demonstrate this using the command sleep, which does nothing but keep the terminal session busy for a set number of seconds. During that interval of time (i.e., before the command completes), we press the Control and C keys simultaneously, to manually interrupt the command.\n  "}).add({id:25,href:"/Help/docs/linux/essentials/commands/",title:"Helpful commands",description:"A cheatsheet of helpful Bash commands.",content:"Overview # This page introduces commands that are completely safe and purely display information that may be used by users to get help or explore the environment of the CCB cluster\necho # The echo command can be used to print a message or the value of a variable.\nFor instance:\necho 'Hello, world!'    man # The man \u0026lt;page\u0026gt; command can be used to print the reference manual page \u0026lt;page\u0026gt;.\nIn many cases, \u0026lt;page\u0026gt; is the name of the command that you wish to execute.\nFor instance, the reference manual page for the command pwd can be displayed as follows:\nman pwd    ðŸ‘‰  The manual page is open in an interactive viewer in the Terminal application. To close that interactive viewer, press the 'Q' key.   Incidentally, the reference manual page for the command man is opened as follows:\nman man  The ability to access, read, and understand manual pages is one of the first essential skills to develop as a Bash user.\nðŸ‘‰  A lot of information and advice on Bash commands, tips and tricks, is available throughout the Internet. However, the manual pages accessed through the 'man' command correspond to the current version of each command available on the CCB cluster, and should be considered as the absolute reference above any other source of information.   type # The type \u0026lt;name\u0026gt; command can be used to identify how \u0026lt;name\u0026gt; would be interpreted if used as a command.\ntype pwd type less    Identifying commands that are Bash built-in or additional command from third-party software can be helpful to understand and investigate bugs and unexpected behaviours.\nwhich # The which \u0026lt;program\u0026gt; command can be used to display the full path to the command \u0026lt;program\u0026gt;.\nwhich man    groups # The groups command can be used to print the user groups that a user belongs to.\nAlone, the groups command display the list of user groups that current user belongs to.\ngroups  The groups \u0026lt;username\u0026gt; command can also be given a username, in which case it will display the list of user groups that particular user belongs to.\nrealpath # The realpath command can be used to resolve the absolute file path to a specific file or directory in the filesystem.\nIn many cases, files and directories may be referred to through any valid path. However, in some cases, it may be necessary to identify and use the \u0026lsquo;real\u0026rsquo; path to a file or directory, meaning the unambiguous absolute path to that file from the root directory.\nrealpath file1.txt    wc # The wc \u0026lt;file\u0026gt; command can be used to count the number of lines, words, or bytes in files.\nMost commonly, the wc command is used with the option -l to specifically focus on the number of lines in a given file. This is particularly useful for bioinformatics file formats where results are reported one per line.\nFor instance, the SAM file format reports alignments of sequences to a reference genome, one per line, meaning that the number of lines indicates the total number of alignments.\nwc -l file1.txt    Commonly used options for the wc command:\n   Option Long option Description     -c --bytes Prints the byte count.   -m --chars Prints the character count.   -l --lines Prints the line count.   -L --max-line-length Prints the length of the longest line.   -w --words Prints the word count.    "}).add({id:26,href:"/Help/docs/linux/essentials/wildcards/",title:"Wildcards",description:"Using wildcards to specify groups of filenames matching a pattern.",content:"What are wildcards? # Wildcards are special characters that match one or more characters in filenames.\n   Wildcard Meaning     * Matches any sequence of characters.   ? Matches any single character.   [characters] Matches any single character in a set.   [!characters] Matches any single character not in a set.   [[:class:]] Matches any character in the specified class.    Many commands accept groups of files and directories matched using wildcards.\nFor instance:\n The rm command can be used to remove multiple files and directories (remember to use the option -r to remove directories using rm). The ls command can be used to list files and the contents of directories.  Examples # * # The symbol * (asterisk) matches any sequence of one or more characters.\nFor instance:\nls *put.txt    In the example above, the * symbol matched:\n in in the file input.txt out in the file output.txt  ? # The symbol ? (question mark) matches any single character at that position.\nFor instance:\nls file?.txt    In the example above, the ? symbol matched:\n 1 in the file file1.txt 2 in the file file2.txt 3 in the file file3.txt  [characters] # The syntax [ ] (square brackets) can be used to specify a set of characters that can be matched at a given position.\nOnly one of the characters in the set can be matched at the given position, no matter how many characters are present in the set.\nFor instance:\nls file[12].txt    In the example above, the set [12] matched:\n 1 in the file file1.txt 2 in the file file2.txt  [!characters] # A set of characters prefixed with the symbol ! (exclamation mark) can be used to specify a set of character that must not match at the given position.\nFor instance:\nls file[!12].txt    In the example above, the set [!12] matched:\n 3 in the file file3.txt  [[:class:]] # The syntax [[: :]] (two square brackets and a colon symbol) can be used to specify a class of characters that can be matched at a given position.\nCommonly used classes of characters are listed below. Find more on the page Character Classes and Bracket Expressions â†’.\n   Class Description     alnum Alphanumeric characters, including alpha and digit.   alpha Alphabetic characters, including lower and upper.   digit Digits: 0 1 2 3 4 5 6 7 8 9.   lower Lower-case letters: a b c d e f g h i j k l m n o p q r s t u v w x y z.   punct Punctuation characters: ! \u0026quot; # $ % \u0026amp; \u0026rsquo; ( ) * + , - . / : ; \u0026lt; = \u0026gt; ? @ [ \\ ] ^ _ ` { | } ~.   upper Upper-case letters: A B C D E F G H I J K L M N O P Q R S T U V W X Y Z.    For instance:\nls file[[:digit:]].txt    In the example above, the class [[:digit:]] matched:\n 1 in the file file1.txt 2 in the file file2.txt 3 in the file file3.txt  Final words # Multiple wildcards can be used in the same pattern.\nFor instance:\n file*.* file?.*  "}).add({id:27,href:"/Help/docs/linux/files-and-directories/",title:"Files and directories",description:"",content:""}).add({id:28,href:"/Help/docs/linux/files-and-directories/navigate/",title:"Navigate directories",description:"Changing directory and listing the contents of directories.",content:"Print working directory # The working directory is the directory where the Bash session is currently located, and relative to which commands typed in a Terminal are executed.\nThe pwd command prints the current working directory.\npwd    When you log into the CCB cluster, the working directory is initially set to the user\u0026rsquo;s home directory.\nðŸ‘‰  The 'pwd' command is entirely harmless. Do not hesitate to experiment and get comfortable with it!   List directory contents # Listing the contents of directories is essential to identify files that are available for use, as well as directories that can be navigated into.\nThe ls command prints the list of files in a given directory.\nExamples # Alone, the ls command prints the list of files and directories in the working directory.\nls    ðŸ‘‰  Colors are used to indicate the type of each file (directory: dark blue; file: black; symbolic link: light blue).   Given the path to an existing directory, the ls command prints the list of files in that particular directory.\nðŸ‘‰  The directory path can be absolute or relative to the working directory.   ls / # absolute path (starts with '/') ls ~/.ssh # relative path (does not start with '/')  The ls command also accepts a number of options. Most commonly, the -l option is used to display detailed information about each file, including permissions, file size, and the timestamp of the latest update to each file.\nls -l    Often, the -h option is added to display file sizes in human-readable format, adding units (e.g., K - kilobyte, M - megabyte, G - gigabyte). Multiple options can be combined under the same - symbol. The two forms below are equivalent.\nls -lh ls -l -h    The -t option can be used in combination with the -l option to sort files by modification time (newest first).\nls -lt    ðŸ‘‰  The timestamp indicating the more recent edit to each file is displayed as the field directly on the left on each filename.   The -a option can be used to reveal and include hidden files and directories.\nls -a    ðŸ‘‰  Hidden files and directories have a name that start with the '.' symbol, e.g. '.ssh'.   Finally \u0026ndash; to clarify \u0026ndash; options and paths can be combined, to list the contents of a particular directory with specific options.\nls -ltah ~/.ssh    ðŸ‘‰  The 'ls' command is entirely harmless. Do not hesitate to experiment and get comfortable with it!   Cheatsheet # Common options for the ls command are listed below, in alphabetical order of the option flag.\n   Option Long option Descrition     -a --all Display all files including hidden files.   -d --directory Display information about a directory instead of listing the contents of that directory. Usually combined with option -l.   -F --classify Append an indicator to the end of each listed name (e.g., / for a directory)   -h --human-readable Combine with option -l to display file sizes in human readable format rather than bytes.   -l  Display information in long format.   -r --reverse Display results in reverse order. See also options -S and -t   -s  Sort by file size.   -t  Sort by latest modification time.    Change directory # Changing the working directory is often essential to run commands in the appropriate working directory.\nGiven the path to an existing directory, the cd command changes the working directory to that directory.\ncd / # absolute path (starts with '/') cd ~/.ssh # relative path (does not start with '/')    ðŸ‘‰  The directory path can be absolute or relative to the current working directory.   Alone, the cd command changes the working directory to the user\u0026rsquo;s home directory.\ncd    ðŸ‘‰  Use the 'ls' command to identify existing directories that you give to the 'cd' command.   The .. shorthand refers to the parent directory of each directory on the system. This can be used to move out of a directory. The shorthand can be combined multiple times in the same path to move out multiple levels at once. Some examples are given below.\ncd .. cd ../.. cd ../../another_directory  ðŸ‘‰  The 'cd' command is entirely harmless. Do not hesitate to experiment and get comfortable with it!   Final advice # Use Autocompletion!\nBriefly, the tabulation key (TAB) may be pressed after typing the first few characters of the name of a valid directory, allowing the Bash session to predict and automatically complete the name of the directory.\nAutocompletion is automatically available for file paths on the CCB cluster, saves a lot of manual typing, and avoid typographical errors that often go unnoticed and raise errors when executed.\nThis process can be repeated multiple times within the same command, even within the same path: type a few characters, press TAB, press type a few more characters, press TAB, etc.\nThere are two main reasons why nothing might happen when you press TAB while autocompleting the path to a directory or file:\n The path that you have typed so far does not exist. The path that you have typed so far is ambiguous.  The easiest way to diagnose which scneario you are in is to press TAB a second time.\nIf a number of options appear, matching the characters that you typed so far, it means that the path that you have typed so far is ambiguous. You need to type more characters manually to disambiguate the path before attempting to use autocompletion again.\n  If nothing appears after having pressed TAB twice, then you likely are in the first scenario: the path that you have typed so far does not exist. You might want to proofread what you typed so far, but the fastest way to fix the path is to type it all from scratch again (using autocompletion to avoid typographical errors).\n"}).add({id:29,href:"/Help/docs/linux/files-and-directories/permissions/",title:"Files and permissions",description:"A brief introduction to working with files and managing permissions on the CCB cluster.",content:"Example scenario # In this page, examples refer to the layout of files and directories illustrated in the schematic below.\n  Absolute and relative paths # Absolute file paths start from the \u0026lsquo;root\u0026rsquo; directory, represented by the / symbol.\nFor instance, in the example scenario illustrated above, the following file and directory paths are valid:\n/ /bin /bin/bash /usr/local /usr/local/bin  Relative file paths describe paths relative to the working directory. As such, they start with any valid character other than /, either referring to the name of a sub-directory, or using the .. shortcut referring to the parent directory.\nFor instance, in the illustration above, the following file and directory paths are valid, relative to the working directory /usr:\nbin include local local/bin ../bin ../bin/bash ../etc/crontab  ðŸ‘‰  The '..' shortcut can be used multiple times in the same path, to navigate multiple levels closer to the 'root' directory (e.g., '../../bin').   Both absolute and relative paths can be used in Bash commands.\nFor instance:\ncd /usr/local/bin  Shortcuts #  The symbol . (full stop) refers to the current directory. The shortcut .. (two consecutive full stop symbols) refers to the parent directory. The symbol ~ (tilde) refers to the home directory of the current user. The / symbol (forward slash) refers to the root directory of the filesystem.  The working directory # The current working directory is always indicated in the prompt of the Linux shell.\nIn the example below, the prompt initially indicates the working directory to be the home directory (represented by the symbol ~, highlighted in red below). After using the cd command to change directory, the prompt then indicates the new working directory.\n  File permissions # File permissions are critical to ensure that access to individual files and directories is restricted to the appropriate users.\nEach file is owned by exactly one user (indicated in the the column highlighted as \u0026lsquo;username\u0026rsquo; below).\nMoreover, each user can belong to a number of user groups, and each file is assigned to exactly one user group (indicated in the the column highlighted as \u0026lsquo;usergroup\u0026rsquo; below) that can be given its own set of permissions on that file.\nðŸ‘‰  To list the user groups that you belong to, use the command 'groups'.   File permissions can be displayed using the ls -l command. In the example below, the options -a and -F are added to display hidden files and append a symbol indicating the type of each file.\n  For a single file, permissions are indicated as a sequence of characters (e.g. drwxrwxr-x).\nThe first character is either d if the file is a directory, or - if it is a regular file.\nThe remaining sequence of characters is read in triplets that represent permissions for three groups of users:\n first triplet: the user who owns the file second triplet: the user group assigned to the file third triplet: every other user who is not the file owner and does not belong to the user group assigned to the file.  Each triplet is interpreted as follows:\n The character r indicates that the corresponding user(s) have read access to the file (i.e., they can open the file). The character w indicates that the corresponding user(s) have write access to the file (i.e., they can edit the file). The character x indicates that the corresponding user(s) have execute access to the file (i.e., they can run the file as a program). The character - indicates that the corresponding user(s) do not have the corresponding permission on that file.  For directories, the permission are better described as follows:\n The \u0026lsquo;read\u0026rsquo; permission indicates the permission to list the contents of the directory. The \u0026lsquo;write\u0026rsquo; permission indicates the permission to create or edit files in that directory. The \u0026rsquo;execute\u0026rsquo; permission indicates the permission to navigate (i.e., change directory) into that directory.  Changing file permissions # The chmod \u0026lt;permissions\u0026gt; \u0026lt;file_or_directory\u0026gt; command can be used to modify the permissions on individual files and directories.\nMost commonly, changes in file permissions are made to:\n Make script files executable. Make valuable files (e.g., raw sequencing data) read-only, to avoid accidentally deleting them.  ðŸ›‘  Do not change file permissions to give other users access to your own files and directories. Instead, contact the system administrators to request project directories that are set up with shared access for collaborators.   File permissions can be described in two formats:\n the alphabetical notation, which is more verbose, but often easier to read and write. the octal notation, which is more compact, but often takes more practice to read and write.  Using alphabetical notation # To change file permissions using the alphabetical notation, the chmod command takes two arguments:\n the permissions to modify the set of files and directories affected by those changes  The permissions to modify are declared using the following syntax:\n u for user, g for group, o for other - for removing permissions, + for adding permissions r for read, w for write, x for execute permission , for separating multiple sets of changes  For instance, the example below illustrates how the chmod command can be used to simultaneously:\n add execute permission for the user who owns the file (i.e., u+x) remove write permission for the user group assigned to the file (i.e., g-w) remove read permission for every other user (i.e., o-r)  chmod u+x,g-w,o-r file1.txt    When the same permissions are applied to multiple groups, the command may be simplified by combining the characters representing those groups together.\nIn the example below, read, write, and execute permissions are all removed simultaneously from both the user group and other users:\nchmod go-rwx file1.txt  Using octal notation # To change file permissions using the octal notation, the chmod command takes two arguments:\n the new set of permissions the set of files and directories affected by those changes  The new set of permissions are declared using the following syntax:\n a triplet of digits indicates the new permission for each of the three types of users (owner, group, other). a value of 4 indicates read permission a value of 2 indicates write permission a value of 1 indicates execute permission for each type of user, values are added together to make a digit between 0 (no permission) and 7 (all permissions).  For instance, the example below illustrates how the chmod command can be used to simultaneously:\n give read, write, and execute permission to the owner of that file give only read permission to the group assigned to that file remove all permissions to other users on that file  chmod 740 file1.txt    "}).add({id:30,href:"/Help/docs/linux/files-and-directories/view-and-edit/",title:"Viewing and editing files",description:"Viewing and editing files in a Linux shell.",content:"Display the contents of a file # Full contents # The cat \u0026lt;file\u0026gt; command can be used to display the entire contents of a file in the Terminal application.\nFor instance:\ncat file2.csv    ðŸ‘‰  While the 'cat' command is entirely harmless, it is highly inefficient for very large files and may freeze the Terminal application. If that happens, you may interrupt the command using the 'Control' and 'C' keyboard shortcut.   First lines # The head \u0026lt;file\u0026gt; command be used to display the first few lines of a file.\nThe number of lines displayed can be controlled using the -n option. In the absence of option, the first 10 lines are shown by default.\nExample usage:\nhead file2.csv head -n 3 file2.csv    Last lines # The tail \u0026lt;file\u0026gt; command be used to display the last few lines of a file.\nThis command works very much like the head command.\ntail -n 3 file2.csv    Interactively scroll through files # more # The more \u0026lt;file\u0026gt; command can be used to scroll through a file unidirectionally from top to bottom.\n  While the viewer is active in a Terminal application, pressing the Space bar scrolls down one screen worth down the contents of the file.\nIn the bottom left corner of the Terminal application, a status bar indicates the fraction of the file that has been scrolled through.\nOnce the end the file is reached, the interactive viewer automatically terminates and returns the Linux prompt to the user.\nAlternatively, pressing the Q key can be used to close the interactive viewer before the end of the file is reached.\nless # The less \u0026lt;file\u0026gt; command provides similar yet more extensive functionality over the more command (\u0026ldquo;less is more\u0026rdquo;).\n  While the viewer is active, the Up and Down arrow keys can be used to scroll one line up or down the contents of the file.\nThe Left and Right arrow keys can be used to scroll one screen worth left or right across the contents of the file.\nSimilarly to the more command, the Space bar can be used to scroll one screen worth down the contents of the file.\nThe Q key must be pressed to close the interactive viewer (reaching the end of the file will not automatically close the interactive viewer).\nEditing files in the Terminal # nano # The nano \u0026lt;file\u0026gt; command can be used to open an interactive text editor in the Terminal application.\nnano file2.csv    In particular:\n The arrow keys can be used to move the cursor through the file. Common keyboard shortcuts are displayed at the bottom of the editor. The ^ symbol represents the Control key. For instance, ^X indicates that pressing the Control and X keys simultaneously will exit the interactive text editor.  To save edits made to a file:\n Press the Control and X keys simultaneously to initiate the exit from the editor. When prompted whether to \u0026lsquo;Save modified buffer\u0026rsquo;, press Y to confirm. When prompted for the \u0026lsquo;File Name to Write\u0026rsquo;, immediately press the Return key to use the current file name.    To close a file without saving the edits:\n Press the Control and X keys simultaneously to initiate the exit from the editor. When prompted whether to \u0026lsquo;Save modified buffer\u0026rsquo;, press N to discard the changes.  To save the new version of the file under a different name:\n Press the Control and X keys simultaneously to initiate the exit from the editor. When prompted whether to \u0026lsquo;Save modified buffer\u0026rsquo;, press Y to confirm. When prompted for the \u0026lsquo;File Name to Write\u0026rsquo;, edit the file name as needed, and press the Return key to write the modified file under the new filename. The original file will be left unchanged.  emacs # The emacs \u0026lt;file\u0026gt; command can be used to open an interactive text editor in the Terminal application.\n  In particular:\n The arrow keys can be used to move the cursor through the file. The GNU Emacs Reference Card list many keyboard shortcuts that make Emacs one of the most efficient text editors in Terminal applications.  To save edits made to a file:\n Press the Control and X keys simultaneously to initiate a command, then Control and S simultaneously to save the file.    ðŸ‘‰  In the screenshot above, the command prompt circled in red displays 'C-x-' to indicate that the keys 'Control' and 'X' were pressed simultaneously, and that the prompt await another keyboard shortcut to complete the command.   To close a file:\n Press the Control and X keys simultaneously to initiate a command, then Control and C simultaneously to close the file.  ðŸ‘‰  To save changes and exit the editor, successively follow the instructions to first save edits, and then instructions to close the file. To close the editor without saving changes, only follow the instructions to close a file.   vim # The vim \u0026lt;file\u0026gt; command can be used to open an interactive text editor in the Terminal application.\n  In particular:\n The arrow keys can be used to move the cursor through the file. The editor initially opens the file in Read-only mode. To edit the file, press the I key to enter Edit mode. To disable Edit mode, press the Esc key to return to Read-only mode.  To save edits made to a file:\n Press the : key to open a prompt for commands within the editor. Then, type wq to write the file and quit the application (press the Return key to execute the command).    To close a file without saving the edits:\n Press the : key to open a prompt for commands within the editor. Then, type q! force quit the application (the ! symbol forces the command to override warnings of unsaved changes).  Creating new files # Text editors such as nano, emacs, and vim can be used to create new files, immediately opening those files in an interactive text editor.\nTo create new files, those commands must be given a filename that does not exist yet.\nFor instance:\nnano new_file.txt    "}).add({id:31,href:"/Help/docs/linux/files-and-directories/manage/",title:"Manage files and directories",description:"Creating, renaming, and removing files and directories.",content:"Creating empty files # The touch \u0026lt;file\u0026gt; command can be used to create new empty files.\nMultiple files can be created in a single command.\nFor instance:\ntouch a.txt b.csv    ðŸ‘‰  In the screenshot above, the zeroes highlighted in red indicate that the new files are empty (i.e., size of 0 bytes).   If a file already exists, the touch command does not edit the contents of the file, but update the timestamp of the latest edit to the current date-time.\nCreating directories # The mkdir command can be used to create new directories.\nMultiple directories can be created in a single command.\nFor instance:\nmkdir dir1 dir2    Copying files # The cp command can be used to make a copy of a file.\nThe command requires two arguments:\n first, the filepath to the original file second, the path to the file where the copy must be made  For instance:\ncp a.txt a_copy.txt  ðŸ‘‰  If the filename of the copy already exists, that file will be overwritten.   Copying directories # The cp command can be used with the option -r to recursively copy a directory and its contents to a new directory.\nSimilarly to the copy of files the command requires two arguments:\n first, the location of the original directory second, the path to the new directory to create as a copy.  ðŸ‘‰  If the directory to create already exists, the copy will be created inside that existing directory.   Moving and renaming files # The mv command can be used to both move and rename files.\nIn particular, renaming a file can be seen as moving the file from one name to another, within the same directory. Meanwhile, a file can be moved to another directory, either keeping the name of the file or renaming the file at the same time.\nFor instance, a file file1.txt can be renamed to file1.csv as follows:\nmv file1.txt file1.csv    On the other hand, moving a file file1.txt to another directory \u0026ndash; without renaming the file itself \u0026ndash; can be done as follows:\nmv file1.txt ../another_demo_dir/    Remember that absolute paths may be used, both for the file to move and the directory to move it into:\nmv /path/to/file1.txt /path/to/new/directory  Finally, a file can be moved to another directory and renamed in a single command as follows:\nmv file1.txt ../another_demo_dir/file1.csv  ðŸ‘‰  If the destination file path already exists, the file at that path will be overwritten by the file that is moved in its place.   Moving and renaming directories # Similarly to files, directories can be moved and renamed using the mv command.\nFor instance, a directory dir1 can be renamed to dir2 as follows:\nmv dir1 dir2    ðŸ‘‰  The name of the directory has changed from 'dir1' to 'dir2', while the contents of the directory remain the same.   However, if the the new directory already exist, the Bash session will then interpret the command as a request to move the first directory into the second one.\nmv dir1 dir2    ðŸ‘‰  The directory 'dir1' is now located inside the directory 'dir2'.   Removing files and directories # The rm command can be used to remove files and directories.\nFiles # For files, the rm command takes the name(s) of one or more file(s) to remove.\nrm file1.txt    Multiple files can be removed in a single command, stating individual filenames and/or using the wildcard symbol *.\nFor instance, the next command demonstrates how to remove the files named file1.txt and file1.csv, along with all files whose names start with file2 and all files that end with .csv:\nrm file1.txt file1.csv file2* *.csv  Directories # For directories, the rm command must be used with the option -r.\nThe option -r indicates that the directory must be remove recursively, meaning that any content in the directory will be removed as well. Refer to the rmdir command below for a safer alternative.\nrm -r dir1    ðŸ‘‰  The option '-r' must be used even if the directory is empty.   ðŸ›‘  Do not use the 'rm -r' before first running the corresponding 'ls -R' command and checking the contents of the directory that you are about to remove. We make this recommendation even more strongly when wildcard symbols are used.   It is possible to verbosely display the list of files and directories that are removed by the rm command using the option -v.\n  ðŸ‘‰  While the option '-v' can be useful to display and record the name of files and directories that were deleted, it will not help you recover files that were deleted unintentionally. Files are deleted immediately and permanently.   As a safer alternative, the rmdir command can be used to remove only empty directories. The command will raise an error when attempting to remove a directories that are not empty.\nIn the example below, the rmdir command is asked to remove two directories. However, one of those directories is not empty, and only the empty directory is actually removed.\nrmdir dir1 dir2    Cheatsheet # Common options for the rm command are listed below, in alphabetical order of the option flag.\n   Option Long option Descrition     -i --interactive Interactively prompt the user for confirmation before deleting files. Without this option, files are silently deleted.   -r --recursive Recursively delete directories. This option is required to delete directories.   -f --force Do not prompt and ignore errors due to nonexistent files. This overrides the -i option. Use with extreme care.   -v --verbose Display informative messages as deletions are performed.    "}).add({id:32,href:"/Help/docs/linux/files-and-directories/links/",title:"Link to files and directories",description:"Create and manage links to files and directories.",content:"Hard links and soft links # Definitions # A hard link is an additional name for an existing file.\nA symbolic link (also called soft link or abbreviated symlinks) is a file that contains a pointer to a target file or directory.\nConsequences # Deleting all the hard links to a file will effectively delete the file itself, as there will not be any path leading to that file in the filesystem anymore. As long as one hard link exist, the file will continue to exist and remain accessible (subject to read, write, and execute permission).\nSymbolic links overcome two major disadvantages of hard links:\n Hard links cannot span across physical devices (i.e., disks). The CCB cluster is composed of many nodes and disks, making hard links unsuitable to many situations. Hard links cannot reference directories but only files.  Symbolic links become unusable if the original file is moved or deleted. That is, unless the symbolic link is updated to point to the new location of the target file.\nSymbolic links can point to target files using absolute or relative paths (relative to the location of the symbolic link itself). Best practices often depend on whether the symbolic link itself is expected to move at any point in the future:\n Symbolic link that use absolute paths will continue to point to the same location even if the symbolic link itself is moved. Symbolic link that use relative paths will point to a new location if the symbolic link itself is moved, relative to the new location of the symbolic link.  Creating hard links # The ln command can be used to create links.\nHard links are created when the option -s is not used.\nThe command must be given the name of the (existing) target file first, and the name of the link second.\nFor instance, a hard link to a file file1.txt named link1.csv can be created as follows:\nln file1.txt link1.csv    ðŸ›‘  Do not create hard links on the CCB cluster. If you think you need a hard link, contact system administrators first to enquire.   Creating soft links # Soft links are created using the same command ln as hard links, while adding the option -s.\nFor instance, a soft link to a file file1.txt named link1.csv can be created as follows:\nln -s file1.txt link1.csv    Similarly, the example below demonstrates the creation of a soft link named link1 to a directory named dir1.\nln -s dir1 link1    "}).add({id:33,href:"/Help/docs/linux/files-and-directories/file-types-and-extensions/",title:"File types and extensions",description:"Understand file types and extensions.",content:"Plain text and binary files # File contents can be categorised in two major types:\n Plain text Binary  Plain text files contain only characters of readable materials that can be displayed by most text editors. However, plain text files cannot contain graphical representations, nor other computational objects (e.g., images, sounds).\nBinary files describe computer files that are not plain text. Many different types of binary file formats are used to store virtually any type of of file content whatsoever (e.g., compiled computer programs, images, sounds, formatted documents).\nIn bioinformatics, binary file formats are commonly used to store compressed versions of equivalent plain text files (e.g., the SAM and BAM file formats).\nFile extensions # File extensions are suffixes appended to the end of filenames, to indicate the file format used in that particular file.\nMany programs make use of file extensions in their input and output file names:\n To parse the contents of input files according to their format. To indicate the format used to write the contents of their output files.  ðŸ‘‰  Even when programs do not make use of file extensions, we recommend using an appropriate file extension when naming your files. Even the generic '.txt' file extension is a good habit to indicate that the file contains plain text information.   Many file extensions have been \u0026ndash; and continue to be \u0026ndash; created to describe plain text file formats that structure their contents differently.\nExamples of bioinformatics file formats include, among many others:\n .fasta \u0026ndash; biological sequence information. .fastq \u0026ndash; sequence information with quality scores. .sam \u0026ndash; alignments of sequences to a reference genome. .gtf, .bed \u0026ndash; genomic coordinates of sequence features (e.g., exons, peaks).  ðŸ‘‰  Renaming a file and changing its file extension will not cause any error if the new file extension does not match the format of the contents. A mismatch between file extension and contents may only be detected when the file is actively used by a program that relies on the file extension and subsequently fails to parse the contents of the file.   "}).add({id:34,href:"/Help/docs/linux/working/",title:"Working in Linux",description:"",content:""}).add({id:35,href:"/Help/docs/linux/working/streams/",title:"Linux streams",description:"Using streams to pass information to and from files.",content:"What are streams? # Streams are mechanisms to pass data from one file or command to another.\nFor instance, a few common scenarios that use streams:\n passing the contents \u0026ndash; not the name ! \u0026ndash; of a file as the input to a command passing the output of a command as the input to another command writing the output of a command to a (new or existing) file  Some standard streams are commonly referred to in Linux:\n standard input (stdin): the default device (i.e., resource) from which input to the system is taken. standard output (stdout): the default file descriptor where a process (i.e., command) can write output standard error (stderr): the default file descriptor where a process (i.e., command) can write error messages  By default, both stdout and stderr write to the terminal. However, it is possible to redirect their respective output to separate destinations (most commonly, files).\nRedirecting streams # Streams can be redirected to new destinations \u0026ndash; including files \u0026ndash; using the symbols \u0026gt;, \u0026lt;, and variants thereof.\nStandard input # The contents of a file can be redirected to the standard input of a command using the syntax command \u0026lt; file.\nFor instance:\nhead \u0026lt; file2.csv    Standard output # Conversely, the standard output of a command can be redirected to a file using the syntax command \u0026gt; file.\nFor instance:\necho \u0026quot;Some text\u0026quot; \u0026gt; file.txt    ðŸ‘‰  The symbol '' overwrites the output file with the new contents. See the operator '' in the following section for appending contents to a file.   Appending to a file # In contrast to the single symbol \u0026gt; which overwrites the contents of a file with the standard output of a command, the operator \u0026gt;\u0026gt; can be used to append that standard output to the file, using the syntax command \u0026gt;\u0026gt; file.\nFor instance:\necho \u0026quot;Some more text\u0026quot; \u0026gt;\u0026gt; file.txt    Standard error # The standard output of a command can be redirected to a file using the operator 2\u0026gt;in the syntax command 2\u0026gt; file.\nFor instance:\nls missing 2\u0026gt; stderr.txt    In the example above, the error message is written and saved to a file, instead of being displayed in the Terminal application and lost when the application is closed.\nGeneral advice # Redirecting the standard error \u0026ndash; and the standard output, for that matter \u0026ndash; can be extremely helpful to investigate and diagnose errors and bugs. Many command produce significant amounts of information on the standard output and standard error, making it a real challenge to identify the source of unexpected issues while scrolling through many screens worth of information.\nIn particular, redirecting the standard output and standard error to different files is highly recommended, to separate generally informative messages that report on the normal functioning of a program from important error messages that highlight unexpected issues.\nFinally \u0026ndash; and confusingly \u0026ndash; some programs are known to misuse the standard output and standard error streams; most frequently, writing informative messages in the standard error stream. Unfortunately, there is nothing that users can do about this, apart from identifying those programs and getting used to where they can find relevant information.\n"}).add({id:36,href:"/Help/docs/linux/working/pipes/",title:"The Linux pipe",description:"Using the Linux pipe to pass information between commands.",content:"What is the Linux pipe? # The Linux philosophy is that one tool should only perform only one task, in a way that complex workflows may be broken down into smaller tasks that can be resolved by combining multiple tools.\nThe Linux pipe is represented by the symbol | and provides a tool connecting the standard output of one command to the standard input of another command.\nUsing the Linux pipe # For instance, the standard output of a command command1 can be passed to the standard input of a command command2 using the sytax command1 | command2.\nðŸ‘‰  The Linux pipe is read from left to right. The standard output of the command on the left is passed to the standard input of the command on the right.   For instance:\ncat file1.txt | head -n 5    "}).add({id:37,href:"/Help/docs/linux/working/combine-commands/",title:"Combine commands",description:"Declare successive or conditional sets of commands.",content:"Successive commands # Generally, successive commands are executed in separate statements.\nFor instance:\necho \u0026quot;Some text\u0026quot; echo \u0026quot;Some more text\u0026quot;    However, the symbol ; can be used to combine multiple commands in a single statement using the syntax command1; command2.\nFor instance:\necho \u0026quot;Some text\u0026quot;; echo \u0026quot;Some more text\u0026quot;    The statements are executed in order, from left to right.\nWhile there is no major preference toward either syntax, a noticeable difference between the two approaches is that:\n Separate statements visibly separate the output of the two commands. A new prompt is visible for each new command, marking the end of the output for one command and the start of the output for the next command. Commands combined into a single statement do not demarcate the output of consecutive commands.  Conditional combinations of commands # Commands can be combined logically, to conditionally execute later commands, depending on the successful completion of earlier commands.\nThe operator \u0026amp;\u0026amp; can be used to execute a command only if the previous command completed succesfully, using the syntax command1 \u0026amp;\u0026amp; command2.\nThe example below illustrates the two scenarios, in which the first command either completes successfully (first) or fails (second):\nwhich echo \u0026amp;\u0026amp; echo \u0026quot;Some text\u0026quot; ls missing \u0026amp;\u0026amp; echo \u0026quot;Some more text\u0026quot;    Conversely, the operator || cab be used to execute a command only if the previous command fails.\nwhich echo || echo \u0026quot;Some text\u0026quot; ls missing || echo \u0026quot;Some more text\u0026quot;    "}).add({id:38,href:"/Help/docs/linux/working/compress/",title:"Compress and decompress files",description:"Use gzip, gunzip, and tar to compress, decompress, and archive files.",content:"Why compress files? # Raw data files and files created during analyses can be large (up to hundreds of GB).\nCompressing files is an efficient way to save disk space.\nCompress using gzip # The gzip command can be used to compress files.\ngzip file1.txt    By default, the original file is removed, and the file extension .gz is added to the original file name, to form the name of the new compressed file.\nDecompress using gunzip # The gunzip command can be used to decompress files that were compressed using the gzip command.\ngunzip file1.txt.gz    By default, the original file is removed, and the file extension .gz is removed from the original file name, to form the name of the new compressed file.\nRedirect to standard output # The option -c can be used in both commands gzip and gunzip for major benefits:\n The original files are kept unchanged (i.e., not deleted). The compressed or decompressed output is redirected to the standard output of the command, meaning that the symbol \u0026gt; can be used to redirect that standard output to any filename (circumventing the default behaviour of both commands).  For instance:\ngzip -c file1.txt \u0026gt; compressed.txt.gz    Archive using tar # The tar command stands for tape archive.\nIt is an archiving file format that combines multiple files \u0026ndash; and directories \u0026ndash; into a single file, called a tar archive.\nOptionally, tar archives can be further compressed during their creation \u0026ndash; using, for instance, the gzip command.\nThe tar command can be used to create, modify, and extract files that are archived in the .tar format.\nFor instance, a directory and a file can be archived together as follows:\ntar -czvf archive.tar.gz file1.txt dir1    In particular:\n The option -c creates a new archive. The option -z further compresses the archive using the gzip command. The option -v verbosely lists the files processed as they are being archived. The option -f declares the name (and location) of the archive file to create.  Extract from a tar archive # As mentioned above, the tar command can also be used to extract files and directories from a tar archive.\nFor instance:\ntar -xvzf archive.tar.gz    Stream compressed files # The zcat command can be used to stream the contents of compressed files to the standard output or to the standard input of a downstream commands.\nThis method is commonly used to bypass the need for a temporary decompressed copy of the file.\nFor instance:\nzcat file1.txt.gz | head    Interactively scroll through compressed files # The zless command can be used as an equivalent to the less command for compressed files.\nFor instance:\nzless file1.txt.gz    Final words # Many programs support gzip-compressed input files. For those programs, there is no need to decompress the files before use.\n"}).add({id:39,href:"/Help/docs/linux/working/filter/",title:"Filter and sort file contents",description:"Display a subset of the contents of files.",content:"Print lines matching a pattern # The grep command can be used to search files and print only lines that match a given pattern.\nThe pattern to search for must be given as a Regular expression â†’. The regular expression does not always need to include special wildcard characters; it can be as simple as the exact sequence of characters to search for.\nThe example below illustrates how the lines of the file file.txt that contain exactly the word error can be identified and returned to the standard output of the Terminal session.\ngrep error file.txt    Common options for the grep command:\n   Option Long option Description     -i --ignore-case Ignore case (i.e., do not distinguish between upper and lower case characters).   -v --invert-match Invert match. Print lines that do not contain a match.   -c --count Print the count of matches, not the matches themselves.   -l --files-with-matches Print the name of each file that contans a match, not the matches themselves.   -L --files-without-matches Like the -l option, but print the name of files that do not contain a match.   -n --line-number Prefix each match with the number of the line within the file.   -h --no-filename When searching across multiple files, suppress the output of filenames.    The example below illustrates how the grep command can be used to print lines that do not contain the pattern \u0026quot;error\u0026quot;.\ngrep -v error file.txt    Extract columns from a file # The cut command can be used to extract one or more columns from a file.\nIn particular:\n The option -d defines the delimiter that is used to separate columns. In unspecified, tabulation is used as a delimiter. The option -f defines the index (or indices) of the fields to extract.  For instance, the third column of a comma-separated file (comma: ,) can be extracted as follows:\ncut -f 3 -d , file2.csv    Multiple columns can be declared either as a comma-separated list (e.g., 1,2,4,5,6) or using the hyphen - (e.g., 1-2,4-6).\nFor instance:\ncut -f 1,3 -d , file2.csv cut -f 1-3 -d , file2.csv    Sort file contents # The sort command can be used to sort the lines in a file according to the values of one or more columns in each line.\nIn particular:\n The option -t defines the delimiter that is used to separate columns. In unspecified, white space characters (including tabulation) are used as field delimiters. The option --key declares one or more fields (i.e., columns) to use for sorting. This option can be declared multiple times in the same command, to declare multiple fields to sort by, breaking ties in earlier fields using subsequent fields. The sort command is rarely used without the --key option, in which case the whole line is used for sorting.  In its simplest form, the option --key uses the following syntax:\n First, the numeric index indicating the position of the field to use for sorting. This can be given as a single value (e.g., 2), or a comma-separated pair of values indicating the starting field and the ending field indices to use for sorting. Then, one or one or more single-letter ordering options among bdfgiMhnRrV, which override global ordering options for that key. Most commonly, the n option is used to override the default alphanumerical ordering by numerical ordering.  For instance:\nsort -t , --key=2 --key=1n file3.csv    In particular:\n The options -t is used to indicate that the symbol , is used as field delimiter. The first option --key declares that the second field should be used to order lines, in alphebetical order of that field. The second options --key declares that the first field should be used to break ties, in numerical order of that field.  Process duplicated lines # The uniq command can be used to remove duplicated lines, or count the number of occurences of each distinct line in a file.\nOmit repeated lines # The uniq command assumes that the file is sorted and only detects duplicated lines that are adjacent to each other. The command sort is commonly used to sort the contents of a file, passing the standard output of the sort command into the standard input of the uniq command using the Linux pipe |.\nFor instance:\nsort file4.txt | uniq    In particular:\n First, the sort command is used to sort lines in the input file in alphabetical order (default sorting order). Then, the uniq command is used to report the first occurence of each distinct line in the sorted input.  Count unique lines # The option -c prompts the uniq command to print the count of occurences for each distinct line in a sorted input.\nFor instance:\nsort file4.txt | uniq -c    Report unique lines # Instead of reporting the first occurence of each distinct line, the option -u can be used to report only unique lines (i.e., lines that only occur a single time throughout the entire sorted input).\nsort file4.txt | uniq -u    Cheatsheet # Common options for the uniq command are listed below, in alphabetical order of the option flag.\n   Option Long option Descrition     -c --count Prefix lines by the number of (consecutive) occurrences.   -d --repeated Only print duplicate lines, one for each group.   -i --ignore-case Ignore differences in case when comparing.   -u --unique Only print unique lines.    "}).add({id:40,href:"/Help/docs/linux/working/find/",title:"Searching for files",description:"Search for files in a hierarchy or directories using the command line.",content:"Motivation # In the Terminal application, programmatically searching for files or directories that match certain conditions (e.g., naming pattern) is a powerful way to apply given commands to groups of related files in a systematic way.\nThe find command can be used to search for files and/or directories that match certain conditions.\nIn particular:\n The first argument to the find command is typically the path to the directory in which to perform the search. The option -type takes a single character value indicating the type of file that is searched (e.g., f for file; d for directory). The option -name takes a Regular expression indicating a pattern that must be matched in the basename to produce a hit.  Examples are given in the following sections.\nSearch for files # The example below illustrates how the find command can be used to find files \u0026ndash; regular files, not directories \u0026ndash; within the directory ~/ccb_demo, and whose basename end with the file extension .txt.\nfind ~/ccb_demo -type f -name *.txt    In particular:\n The path ~/ccb_demo is given as the first argument, indicating that the search must be performed only within that particular sub-directory of the user home directory. The option -type is given the value f, indicating that only regular files should be reported. The option -name is given the pattern *.txt, where the wildcard * matches anything, as long as the basename of the file ends with .txt.  Search for directories # The example below illustrates how the the find command can be used to find directories within the directory ~/ccb_demo.\nfind ~/ccb_demo -type d    In particular:\n The path ~/ccb_demo is given as the first argument, indicating that the search must be performed only within that particular sub-directory of the user home directory. The option -type is given the value f, indicating that only regular files should be reported. The option -name is not given, meaning that no filter is applied on the basename of directories reported; in that case, all directories are reported.  Execute a command on hits # The option -exec can be used to execute a command on each hit from the search.\nFor instance:\nfind ~/ccb_demo -type d -exec echo directory: {} \\;    In particular:\n The option -exec is used to run the command echo directory: {}, where {} is a placeholder that is replaced by each hit in the search results prior to executing the command. The syntax \\; is essential to mark the end of the command given to the -exec option.  Cheatsheet # Common options for the find command are listed below:\n   Option Descrition     -maxdepth With -maxdepth levels, descend at most levels (a non-negative integer) levels of directories below the command line arguments.   -mindepth With -mindepth levels, do not apply any tests or actions at levels less than levels (a non-negative integer).   -regextype With -regextype type, use a type of regex sytax among the available choices: emacs (this is the default), posix-awk, posix-basic, posix-egrep and posix-extended.   -L Follow symbolic links. When find examines or prints information about files, the information used shall be taken from the properties of the file to which the link points, not from the link itself.    For instance:\nfind ~/ccb_demo -mindepth 1 -maxdepth 3 -regextype posix-extended -L  Common options related to tests (i.e., filters) are listed below:\n   Option Descrition     -type With -type c, returns files of type c. Commonly used types are: d for directory, f for file, and l for symbolic links.   -name With -name pattern, returns files whose basename matches pattern.    Final words # The find command is very powerful, with many options documented in the reference manual page (accessible using the man find command).\nman find  "}).add({id:41,href:"/Help/docs/linux/working/loops/",title:"Loop over items and files",description:"Loops can be used to iterate over lists of items, files and directories.",content:"Motivation # When repeatedly executing the same set of commands on a series of inputs, loops can be used to:\n Define a set of input. Define a set of commands. Execute the set of commands on the set of inputs.  The for loop # The for loop takes a list of inputs, and iterates over each item in the list, repeating a set of commands \u0026ndash; once for each input \u0026ndash; until all inputs are processed.\nInteger over fixed inputs # The for loop can be given a list of inputs to iterate over as a space-separated array of values.\nFor instance:\nfor i in 2 5 3 do echo \u0026quot;start\u0026quot; echo \u0026quot;value of i: '$i'\u0026quot; echo \u0026quot;end\u0026quot; done    In particular:\n The operator for declares the start of the loop definition. The (arbitrary) variable name i is declared as the iterator that will take a different value during each iteration. The values to iterate over are given as a space-separated list. The operator do declares the start of the set of commands to execute during each iteration of the loop. A series of commands print messages in the Terminal. In this example, the first and third messages remain the same for each iteration, while the second message dynamically uses the value of the iterator for each specific iteration. The commands are indented only for visual effect; this is not necessary, but it generally improves the readability of Bash scripts. The operator done declares the end of the set of commands to execute during each iteration of the loop.  Iterate over integer values # The curly braces {} and the seq command can be used to rapidly generate regular sequences of integer values to iterate over.\nThe syntax {start..end} can be used to generate a sequence of integers from start to end in increments of 1.\nThe example below illustrate how this syntax can be used to iterate over the integers from 1 to 3 by increments of 1.\nfor i in {1..3} do echo \u0026quot;value of i: '$i'\u0026quot; done    Furthermore, the extended syntax {start..end..increment} can be used to control the increment to use.\nThe example below illustrate how this syntax can be used to iterate over the integers 1 to 7 by increments of 2.\nfor i in {1..7..2} do echo \u0026quot;value of i: '$i'\u0026quot; done    Similarly, the seq command can be used to the same effect.\nThe example below iterates over the integer values from 1 to 3 by increments of 1.\nfor i in $(seq 1 3) do echo \u0026quot;value of i: '$i'\u0026quot; done  The example below iterates over the integer values from 1 to 7 by increments of 2.\nfor i in $(seq 1 2 7) do echo \u0026quot;value of i: '$i'\u0026quot; done  Iterate over files # The for loop can be used to iterate over files, with the iterator variable taking the value of each filename during each iteration.\nWhile the list of filenames to process can be typed in manually, wildcards are often used to match groups of files by pattern.\nFor instance:\nfor file in *.txt do wc -l $file done    Multiple patterns can be given in a single expression, as each pattern automatically expands into the corresponding list of filenames.\nfor file in *.txt *.csv do wc -l $file done  Final words # When writing a new loop, consider testing it on a small set of inputs before executing it on the full set of inputs, as a small set of inputs will run much more quickly during the testing phase.\nConsider adding commands that display informative messages during the execution of the loop:\n The echo command can be used to print messages on the standard output. This may help for debugging (e.g., identifying the iteration when an issue occured). This may help tracking the progress of the loop (e.g., printing the number of iterations so far, or the name of the file being processed).    "}).add({id:42,href:"/Help/docs/linux/processes/",title:"Linux processes",description:"",content:""}).add({id:43,href:"/Help/docs/linux/processes/introduction/",title:"Introduction to Linux processes",description:"A brief introduction to Linux processes on the CCB cluster.",content:"What is a process? # When you execute a Linux command, the Bash session creates a process.\nA process is a running instance of a program.\nEvery process has a process identifier (often abbreviated pid).\nDuring execution, a process changes between states depending on its environment and circumstances. The most common states are:\n Running \u0026ndash; The process is actively running on the system. Waiting \u0026ndash; The process is waiting for an event to occur or for a system resource. Stopped \u0026ndash; The process has been stopped, usually by receiving a termination signal (either from the user or the system). Zombie \u0026ndash; The process has been halted but it still has an entry in the table of processes.  Foreground and background processes # Foreground # By default, command executed interactively in the Bash session of a Terminal application are run in the foreground.\nFor instance, the code below launches a process that does nothing but wait in the foreground for 10 seconds before terminating itself and returning the prompt to the user:\nsleep 10    In particular:\n A command running in the foreground will not return the Terminal prompt to the user until the command has completed. In other words, the user cannot execute any other command until the command in the foreground has completed. The standard output and standard error are printed in the Terminal application.  Background # Process can be run in the background to continue using the same Terminal instance while the process runs.\nTo run a command in the background, add the symbol \u0026amp; (ampersand) at the end of the command.\nFor instance, the code below launches a process that does nothing but wait 10 seconds in the background before terminating itself. During those 10 seconds, the user is free to continue using the same Terminal for launching other commmands, either in the foreground or background.\nsleep 10 \u0026amp;  When running command in the background, it is best to redirect the standard output and standard error to files \u0026ndash; or suppress them altogether \u0026ndash; as the command would otherwise print its standard output and standard error in the Terminal application.\nFor instance:\necho \u0026quot;An example string\u0026quot; \u0026gt; stdout.txt 2\u0026gt; stderr.txt \u0026amp;  In particular, the code above launches a process that:\n Uses the echo command to print the character string \u0026quot;An example string\u0026quot; to the standard output. Redirects the standard output of the command to the file stdout.txt. Redirects the standard error of the command to the file stderr.txt.  Moving a process from foreground to background # To move a process from the foreground to the background:\n First, press simultaneously the Control and Z keys to pause the process running in the foreground. Then, type bg and press the Return key to send that same process to the background.  Moving a process from background to foreground # To move a process from the background to the foreground, the process depends on whether you have one or more processes running in the background.\nUse the jobs command to check how many processes you are running in the background.\njobs    If you have a single process running in the background:\n Type fg and press the Return key to bring that process to the foreground.  If you have more than one process running in the background:\n Type fg followed by the integer number next to the command that you want to bring to the foreground in the output of the jobs command (e.g., fg 2).  ðŸ‘‰  In the output of the 'jobs' command, the symbol '+' indicates the job that would be brought to the foreground by default; the symbol '-' indicates the second job that would be brought to the foreground by default.   Cheatsheet # Common options for the jobs command are listed below:\n   Option Descrition     -l Show the pid in addition to the basic information.   -p Only show the pid.   -n Only show processes that have changed status since the last notification was printed.   -r Only show running processes.   -s Only show stopped processes.    "}).add({id:44,href:"/Help/docs/linux/processes/monitor-processes/",title:"Monitor processes",description:"A brief introduction to monitoring processes on the CCB cluster.",content:"Snapshot current processes # The ps command can be used to report a snapshot of the current processes.\nps    The output of the ps command will always include at least two entries:\n The bash process that is used to run the current Bash session. The ps process itself, which is being being executed to snapshot currently running processes.  Then, any other process that is currently running will be reported by the ps command.\nFor instance:\nsleep 10 \u0026amp; sleep 20 \u0026amp; sleep 30 \u0026amp; ps    In particular:\n PID \u0026ndash; Process identifier (unique). TTY \u0026ndash; Terminal type. TIME \u0026ndash; CPU runtime. CMD \u0026ndash; Process name.  Real-time monitoring # The commands top and htop can be used to display a dynamic real-time view of a running system.\ntop # The top command can be used to monitor processes and resource usage in real time.\nInformation is displayed as a table that can be filtered and sorted.\nAbove the table, a header displays system-wide information (e.g., total number of processes, memory available).\ntop    htop # The htop command can be used to launch an interactive process viewer.\nSimilarly to the top command, informatio is displayed as a table that can be filtered and sorted, as well as a header that displays system-wide information (e.g., total number of processes, memory available, CPU usage).\nhtop    An annotated example output of the htop command is presented below.\n  Terminate a process # kill # Processed can be terminated (often said \u0026lsquo;killed\u0026rsquo;), either by users \u0026ndash; including system administrators \u0026ndash; or by the system itself.\nCommon reasons for terminating processes include:\n They are taking too long (e.g., froze, entered an infinite loop). They are using too many resources (e.g., memory, open file connections). They are no longer functioning properly.  The kill command can be used to terminate processes specified by their PID (Process IDentifier).\nsleep 30 \u0026amp; ps kill \u0026lt;PID\u0026gt;    killall # The killall command can be used to terminate all processes by name.\nFor instance, the code below launches a series of sleep commands, and then terminates all of them by name, without the need to specify their respective PID.\nsleep 30 \u0026amp; sleep 40 \u0026amp; sleep 50 \u0026amp; ps killall sleep    Cheatsheet # Common options for the ps command are listed below:\n   Option Descrition     -e Select all processes. Identical to -A.   -f Do full-format listing (i.e., displays additional information).   -u \u0026lt;user\u0026gt; This selects the processes whose effective user name or ID is .    For instance, replace \u0026lt;user\u0026gt; by your own username and run the command below to display only your processes in full format:\nps -f -u \u0026lt;username\u0026gt;  "}).add({id:45,href:"/Help/docs/linux/processes/hangup/",title:"Resilience to loss of connection",description:"A brief introduction to running processes immune to loss of connection.",content:"What happens when the connection is lost? # The connection to a Bash session on the CCB cluster may be lost for a number of reasons:\n Loss of internet connection anywhere between your computer and the CCB cluster. Power loss causing an unexpected outage of CCB systems. Loss of connection to the University VPN (if applicable).  When the connection to a Bash session is lost, the HUP (hangup) signal is sent to the Bash session. The HUP signal prompts the Bash session to terminate commands that are currently running, potentially resulting in lost work for users.\nHow to run commands immune to loss of connection? # nohup # The nohup command can be used to run a command that is immune to the HUP signal.\nThe process launched by the nohup command will ignore any HUP signal, and continue running until the command completes.\nBy default, the standard output of the process launched by the nohup command is redirected to a file nohup.out in the working directory, unless the command explicitly redirects the standard output to another file.\nBy default, the standard error of the process launched by the nohup command is not saved. However, the command can explicitly redirect the standard error to a file using the operator 2\u0026gt;.\nThe \u0026amp; symbol is commonly added at the end of nohup command, to launch those commands in the background.\nFor instance:\nnohup wc -l file.txt \u0026gt; stdout.txt 2\u0026gt; stderr.txt \u0026amp;    In particular:\n The nohup command is used to make the command immune to the HUP signal; the command will continue to run even in the event of network connection loss problems. The wc command is used to count the number of lines in the file file.txt. The standard output of the command is redirected to the file stdout.txt. The standard error of the command is redirected to the file stderr.txt.  tmux # The tmux command launches an open-source terminal multiplexer for Unix-like operating systems.\nIt allows multiple terminal sessions to be accessed simultaneously in a single window. It is useful for running more than one command-line program at the same time.\nTo start your first tmux session on the CCB cluster, type tmux in the Terminal application, while logged into the CCB cluster:\ntmux    The green banner at the bottom indicates that the user is now in a tmux session. At this point, users can then work in that terminal session like any other Bash session.\nHowever, a benefit of the tmux session is that it will continue to exist and keep running commands alive even if the connection between the client and the CCB cluster is lost, or users close the Terminal application.\nFor instance:\n Close the Terminal application. If prompted, click \u0026lsquo;Terminate\u0026rsquo; to force-quit the application. Open a new Terminal application. Log into the CCB cluster. Type tmux attach (or, in abbreviated form, tmux a).  tmux attach  The very same tmux session will be attached to the new Bash session and users can pick up their work where they left off.\nSometimes, users may wish to effectively terminate one tmux session and start another (e.g., to reset the state of their Bash session). To do so, terminate the Bash session running in the current tmux session using any of the logout methods described in the section Log out.\nFor instance:\nlogout  Do so until you fully exit the tmux session (i.e., the green banner at the bottom of the Terminal application has disappeared). You should see a the message [exited], indicating that you have exited the tmux session.\n  Then, start a new tmux session using the tmux command:\ntmux  ðŸ‘‰  Avoid creating multiple tmux sessions in parallel. Use 'tmux a' to attach any existing session where possible. Only create a new session using 'tmux' when 'tmux a' returns the message 'no sessions'.     A \u0026lsquo;Tmux Cheat Sheet \u0026amp; Quick Reference\u0026rsquo; is available at https://tmuxcheatsheet.com/.\nFor more details, search search \u0026ldquo;tmux tutorial\u0026rdquo; in a search engine or YouTube.\nEmacs # Emacs is a family of text editors that are characterized by their extensibility.\nThe command emacs launches the application.\nemacs    The Emacs editor can be configured to persist in the event of a connection loss, independently of tmux sessions.\nTo do so, the Emacs daemon \u0026ndash; a program that runs continuously as a background process \u0026ndash; must run in the Bash session.\nTo automatically start the Emacs daemon in every new Bash session, add the following lines to the ~/.bashrc file.\nexport ALTERNATE_EDITOR=\u0026quot;\u0026quot; alias emacs=\u0026quot;emacsclient --tty\u0026quot;  Then, launch the Emacs application.\nemacs  At this point, users can then work in that Emacs session without fear of losing work in the event a connection loss.\nFor instance:\n Type anything in the Emacs application. Close the Terminal application. If prompted, click \u0026lsquo;Terminate\u0026rsquo; to force-quit the application. Open a new Terminal application. Log into the CCB cluster. Type emacs.  The very same Emacs session will be restored in the new Bash session and users can pick up their work where they left off.\nWe do recommend regularly saving your work in the Emacs text editor (i.e., saving the contents of the buffer to a file).\nA â€˜GNU Emacs Reference Cardâ€™ is available at https://www.gnu.org/software/emacs/refcards/pdf/refcard.pdf.\nFor more details, search search \u0026ldquo;emacs tutorial\u0026rdquo; in a search engine or YouTube.\n"}).add({id:46,href:"/Help/docs/linux/modules/",title:"Environment modules",description:"",content:""}).add({id:47,href:"/Help/docs/linux/modules/introduction/",title:"Using environment modules",description:"A brief introduction to environment modules.",content:"What are modules? # Environment modules provide a central installation of software available to multiple users on the same computer system.\nIndividual software modules can be loaded and unloaded easily using the module command, which dynamically modifies the user\u0026rsquo;s shell environment as needed.\nFor each module, configuration files are created and maintained by system administrators.\nAs such, new modules can be requested from system administrators (refer to the Contact section), subject to time and availability.\nðŸ‘‰  New versions of existing modules generally require less time to review and setup than entirely new modules.   List available modules # The module avail command can be used to list modules currently available.\nmodule avail  This command generally displays an uncessarily large list of modules.\nThe command can be given a pattern of module names to search for.\nFor instance:\nmodule avail python    Search modules # The module search command is an alternative method for searching modules by keyword.\nmodule search python    List modules currently loaded # The module list command can be used to list modules that are currently load\nmodule list    Load modules # The commands module load and module add can both be used to load modules (one or more module per command).\nFor instance:\nmodule load fastqc    Multiple versions may be available for the same module name (e.g., fastqc/0.11.5, fastqc/0.11.9).\nIn those cases, we recommend specifying explicly the version that you wish to load.\nFor instance:\nmodule load fastqc/0.11.9    ðŸ‘‰  Some modules depend on each other. The 'module load' command will automatically load dependencies (if they are not loaded already).   Unload modules # The commands module unload and module rm can both be used to unload modules (one or more module per command).\nmodule unload fastqc/0.11.9    ðŸ‘‰  Some modules depend on each other. The 'module unload' command will automatically attempt to unload dependencies (if no other loaded module depends on them).   Purge modules # The command module purge can be used to unload all loaded module and reset everything to the original state.\nðŸ‘‰  In this instance, \"original state\" also reverts any 'module' command executed in the '~/.bashrc' file.   For instance:\nmodule list module load R-base/4.2.0 module list module purge module list  In particular:\n The first call to module list reports a module that was loaded in the user\u0026rsquo;s ~/.bashrc file. The command module purge unloads all modules; both those loaded in the user\u0026rsquo;s ~/.bashrc and those loaded interactively on the command line.    "}).add({id:48,href:"/Help/docs/linux/configuration/",title:"Configuration",description:"",content:""}).add({id:49,href:"/Help/docs/linux/configuration/bashrc/",title:"The .bashrc file",description:"Setting up the .bashrc file.",content:"Motivation # Every time you open a new shell on the CCB cluster, you are given a new session in a Bash environment in your Terminal application.\nIn those new sessions, you may find yourself repetitively typing the same set of commands to set up the environment of that session before you are able to work.\nThe file ~/.bashrc is a Shell script that Bash runs whenever it is started interactively. As such, this file is often edited to include commands that users wish to execute every time they log in, without the need to explicitly type them every time.\nHowever, you want to be mindful of the commands that you add in this file, as the automatic execution of those commands may have unintended consequences on the running of other programs (e.g., remote desktop applications).\nNew accounts on the CCB cluster are created with a initial copy of ~/.bashrc file. It is common for the file to evolve over time, as many programs suggest additions as part of their installation and setup procedures.\nThe ~/.bashrc file is a hidden file \u0026ndash; its name starts with a . (dot) symbol \u0026ndash; so it will only be listed by the ls command with the -a option.\nIn this page, we provide an example that contains elements specific to the WIMM CCB cluster, as well as aliases and optional elements that may not be relevant to all users.\nContents # Please carefully read the comments and review the elements that are relevant to you. You may choose to replace the entire contents of your .bashrc file, or select portions of the example below.\n# .bashrc # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi # User specific aliases and functions alias emacs='emacs -nw' alias R='R --no-save' ### Load environment modules # Load the latest version of Git (system version is old) module load git/2.31.1  ðŸ‘‰  If you decide to edit you '~/.bashrc' file, carefully consider any difference between the contents below and the initial contents of the file and whether you take responsibility for any edit. This documentation may occasionally be out of date with respect to current best practices and the contents of the initial file have been carefully reviewed by system administrators.   Explanation # Comments # First, it is important to clarify that, in the contents above, all the lines that start with the # symbol are purely comments that are only added for information purposes and future reference, without any impact on the functionality of the file.\n/etc/bashrc # The first chunk of code \u0026ndash; repeated below \u0026ndash; executes a script that sets up system-wide functions and aliases. That script is exclusively and safely managed by system administrators, and it is highly recommended to keep this as the first bit of code in your ~/.bashrc file at all times.\nif [ -f /etc/bashrc ]; then . /etc/bashrc fi  ðŸ‘‰  The 'if ... then' block ensures that the file exists before attempting to execute it.   Aliases # The next chunk of code uses the alias command to create aliases. Aliases are essentially shortcuts that condense arbitrarily complex (sequences of) commands as single-word keywords, reducing keystrokes and improving speed and efficiency.\nWhile the alias command can be used to define shortcuts interactively in the Bash session, aliases are most commonly defined in the ~/.bashrc file, to make them available as soon as users open a new shell.\nIn this example:\n We change the meaning of the emacs command to automatically apply the option -nw (synonym to --no-window-system). This force the Emacs editor to open within the terminal rather than attempting to open it as a GUI (i.e., windowed) application. We change the meaning of the R command to automatically apply the option --no-save. This forces the R program to discard the workspace at the end of each session instead of offering to interactively save the workspace to a file. Large workspaces \u0026ndash; that contain many objects or large data sets \u0026ndash; can significantly increase the time that it takes for R sessions to start and end (up to several minutes).  alias emacs='emacs -nw' alias R='R --no-save'  Other common aliases include:\n la, to list all files (including hidden files) in long form, with human readable file sizes, and file type indicators.  alias la='ls -alhF'   Overriding the rm command to force an interactive prompt before every removal.  alias rm='rm -i'  ðŸ‘‰  If you decide to create your own aliases, be mindful of overriding any existing command name, as aliases take precedence over commands with the same name.   Modules # The next chunk of code uses the module command to put a version of the git program on the PATH that is more recent than the version originally installed with the operating system.\nmodule load git/2.31.1  ðŸ›‘  Some modules produce output messages when they are loaded. If you are an X2Go user, do not load those modules from your '~/.bashrc' file, as any output message produced during by the login shell will interfere with X2Go.   "}).add({id:50,href:"/Help/docs/linux/configuration/environment-variables/",title:"Environment variables",description:"A brief introduction to environment variables.",content:"Motivation # A variable is a named place in a computer\u0026rsquo;s memory that stores a particular piece of information (e.g., number, string).\nEnvironment variables are variables defined and available in the shell\u0026rsquo;s environment. Those variables can be accessed by any program that runs in the shell. They can be accessed using the $ (dollar) symbol, followed by the name of the environment variable (e.g., $USER).\nOnce logged into the CCB cluster, a number of environment variables are set in the Bash session. Existing environment variables can be modified, and new environment variables can be defined at runtime, including from the ~/.bashrc file while the login shell is executed.\nSome environment variables are identical for all users on the cluster, other are derived from the username, and users can also set environment variables themselves. Many programs require certain environment variables to be set for their good functioning.\nIn this page, we describe some of the built-in environment variables, with a brief description of their purpose and advice on best practices.\nBuilt-in variables # $HOME # The $HOME environment variable is set for each user to their own home directory.\nðŸ‘‰  The '~' symbol is equivalent to the '$HOME' environment variable.   The echo command can be used to display the value of $HOME.\necho $HOME    The $HOME environment variable can also be used in commands. For instance, users can list the contents of their home directory as follows.\nls $HOME  $USER # The $USER environment variable is set for each user to their own username.\nThe echo command can be used to display the value of $USER.\necho $USER    For instance, users create a directory named after their own username as follows. This is particularly useful to use a consistent naming of personal directories that is guaranteed to avoid any conflict with other usernames.\nmkdir $USER  $PATH # The $PATH environment variable is unquestionably one of the most important environment variables.\nThe $PATH is stored as a colon-separated (: symbol) character string that represents the list of directories that are searched \u0026ndash; in the given order \u0026ndash; to find executable files that are invoked by users and their scripts.\nAs such:\n Commands may fail if the directory where they are stored is not listed in the $PATH. Users may need to modify their $PATH to add directories that contain commands they wish to use. Users may need to modify their $PATH to remove directories that contain command they do not wish to use (e.g., when a command is defined in multiple directories).  echo $PATH    Whenever possible, we recommend using the module command to add and remove programs (i.e., modules) from the $PATH.\nFor instance:\nmodule rm git/2.31.1    As an alternative, users may not need to modify their $PATH if they provide the full path to the executable file that they wish to run. However, this approach requires many more keystrokes, and is more prone to typographical mistakes.\nFor instance:\n/package/git/2.31.1/libexec/git-core/git status  $TMPDIR # The $TMPDIR environment variable is set globally to the location of a directory that user should use to store temporary files.\nFor instance, users can create new temporary directories and files in that directory as follows:\nmkdir $TMPDIR/new_temp_dir touch $TMPDIR/new_temp_file    Note that $TMPDIR is the same for all users. To avoid clashes of directories and file names with other users, we recommend creating and using the subdirectory with your own username first.\nFor instance:\nmkdir $TMPDIR/$USER mkdir $TMPDIR/$USER/new_temp_dir touch $TMPDIR/$USER/new_temp_dir/new_temp_file  Custom variables # Motivation # Environment variables are not restricted to built-in variable names. Users can define any number of custom environment variable names.\nSome programs expect users to define additional environment variables, controlling the behaviour of the program without passing arguments explicitly on the command line.\nExpert users may also define custom environment variables for their own purposes (e.g., store a value commonly used on the command line or in their own scripts).\n$TMP # The $TMP environment variable is not set by default.\nSome programs may expect the value of this variable to be a character string indicating the location of an existing directory that can be used to store temporary files during their execution.\nWe recommend setting the value of $TMP to $TMPDIR.\nWe also recommend defining that environment variable in the ~/.bashrc file, so that the environment variable is immediately set in every Bash session.\nIn the example below, we define $TMP interactively at the prompt only for demonstration purposes:\nexport TMP=\u0026quot;$TMPDIR\u0026quot;    "}).add({id:51,href:"/Help/docs/linux/configuration/advanced/",title:"Advanced configuration",description:"Demonstrating advanced notions of Linux configuration.",content:".bashrc # Alternative login shells # The ~/.bashrc file is a script that is only read by the Bash login shell. The CCB cluster using Bash, this login shell should be the only one that you edit and worry about.\nYou may come across mention of other login shells (e.g., ~/.profile, ~/.bash_profile).\nThose other files perform a task similar to the ~/.bashrc file, albeit with subtle differences with respect to the order and the environment in which they are executed.\nWe strongly discourage users from editing \u0026ndash; or creating \u0026ndash; those other login shells. Instead, we recommend that users only edit their ~/.bashrc file.\nInteractive sessions # The PS1 environment variable\nNext, an if statement is used to ensure that certain commands are only executed in interactive Bash session.\nFor instance, interactive session are those that are launched in your Terminal every time that you log into the CCB cluster, in contrast to non-interactive session that are launched when you submit jobs to the queue manager on the cluster (more on cluster jobs in a later section of this documentation).\nif [[ $PS1 ]]; then \u0026lt;... commands ...\u0026gt; fi  # Non-interactive shells inherit the path and other variables # from the calling shell, so this setup is not needed. # prevents conda env being reset when calling P.run() if [[ $PS1 ]]; then # User specific aliases and functions alias emacs='emacs -nw' alias R='R --no-save' # Set umask for default file permissions umask 002 ### Load environment modules # Load the latest version of Git (system version is old) module load git/2.31.1 fi # if PS1  "}).add({id:52,href:"/Help/docs/data-management/organise-and-backup/",title:"Organisation and backup",description:"Organisation and backup doks.",content:""}).add({id:53,href:"/Help/docs/data-management/organise-and-backup/philosophy/",title:"Philosophy",description:"A brief introduction to best practices in data management.",content:"Citation #  \u0026ldquo;Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why.\u0026rdquo;\n\u0026ndash; A Quick Guide to Organizing Computational Biology Projects - William Stafford Noble (Article â†’)\n Overview # In this section, we will discuss:\n Best practices for data storage \u0026amp; organisation. Rules and conventions for naming files. Typical components of genomics data analysis projects. Records metadata and traceability. Backups and archives. Data transfers to and from remote computers.  With a particular focus on genomics data sets and pipelines.\nPrerequisites # Refer to the section Files and directories for essential knowledge and commands to manage files and directories.\n"}).add({id:54,href:"/Help/docs/data-management/organise-and-backup/project/",title:"Collaboration projects",description:"A brief introduction to projects on the CCB cluster.",content:"Overview # Collaboration projects are an easy, secure way for you to share your research data with other people on the cluster. Each project represents a single collaboration and can be as big - or as small - as it needs to be. And you can have as many as you need! Data stored in a project is automatically given the correct permissions so that only other people in the project can see it. You just pick the logical set of data and define the people it needs to be shared with, then we create the project for you and update the list of people as time goes on so that it\u0026rsquo;s always correct. They\u0026rsquo;re a secure, convenient way to share data.\nHow do collaboration projects work? # Each collaboration project has its own project space in the /project/ directory. Hereâ€™s an example of a project with just two members:\n/project/ccbadmin/ /dtooke/ /emacmahon/ /shared/  Each member of the project gets their own directory, which can be used to store project data which doesnâ€™t typically need to be shared; anyone in the project can read it, but only the owner can add or remove data. Each project additionally gets a shared directory which is for data which will be used by multiple people, and anyone can create new directories and files in it. New data which is created in the project space is automatically added to the correct group so that only the project members can see it.\nImportant safety points # It\u0026rsquo;s a common misconception that the shared/ directory allows anyone in the project to read and write the data. This is not the case. The data is still owned by the user who creates it, is charged against their quota and is only writable by them by default. This is deliberate. As much as it may seem attractive to have all data in the directory immediately writable by anyone in the group, that would be dangerous. Multiple people editing the same file at the same time runs a non-trivial chance of unrecoverable data loss and multiple people with edit access to a directory allows any one of them to delete all the files with a single mistyped command. As such, we recommend against trying to \u0026ldquo;fix\u0026rdquo; this.\nHow do I join an existing collaboration project # Please ask the PI in charge of the project to contact us via genmail@molbiol.ox.ac.uk and ask for your username to be added.\nSuccessful projects # There\u0026rsquo;s a few things you can do to maximise the likeihood that your projects will be successful. We\u0026rsquo;d recommend you take the following points into account before you start.\nYou should aim to create a collaboration project for a set of logically connected data, e.g. a specific research project. Creating a collaboration project for a list of people and then thinking about the data they may want to share will risk that you end up with poorly defined data boundries and have to unentangle the data at a future data.\nYou should create a new collaboration project for each logically separate set of data. Adding different data to an existing collaboration project and then adding more people will likely lead to the same problems described above.\nPlease tell us if the permissions on the data in your project arenâ€™t working for you. Trying to work around an issue on your own is more likely to make the problem worse and take longer overall.\nPlease tell us if someone leaves the project and no longer needs access to the data. Leaving accounts with access to data they donâ€™t need is a security and privacy risk.\nHow do I set up a collaboration project # Setting up a new collaboration project is quick and easy.\nðŸ‘‰  Only PIs may request new projects.    Decide the purpose of the project. Each project should be for a single requirement, e.g. data for a lab or data for a research project. If different people need access to different logical data sets, these should be multiple projects. Simmarly, if the same people need access to different logical data sets, these should also be multiple projects. Choose a name for the project that isnâ€™t already in use in /project/. Decide which user accounts should have access to the data. Send an e-mail to genmail@molbiol.ox.ac.uk with the details. Weâ€™ll set up the project and add the members; each member will get a welcome e-mail.  What about my other directories? # The collaboration projects are an addition to your existing options, not a replacement, meaning that you can have several different locations for your data. Our suggestion for organising your data would be the following:\n/home/some_directory/$USER/ : ideal for small private data such as documents or configuration files which are private to you\n/project/project1/$USER/ : used to store data which is part of project1 but wonâ€™t typically be used by other people\n/project/project1/shared/ : used to store data which is part of project1 and is likely to be used by multiple people\nHow does this affect my quota? # Your quota is based on your account. As such, any data which you own is charged against your personal quota, wherever it is. It is not currently possible for us to allocate quota to collaboration projects directly.\nGetting help # You can email the CCB team using the email address genmail@molbiol.ox.ac.uk. Using this address ensures your email is logged and assigned a tracking number, and will go to all the core team, which means the appropriate person or people will be able to pick it up.\nCopyright # This text is copyright University of Oxford and MRC and may not be reproduced or redistributed without permission.\nAuthor # Duncan Tooke (duncan.tooke@imm.ox.ac.uk) and Kevin Rue-Albrecht (kevin.rue-albrecht@imm.ox.ac.uk).\n"}).add({id:55,href:"/Help/docs/data-management/organise-and-backup/filenames/",title:"Name files carefully",description:"A brief introduction to best practices in naming files.",content:"Case-sensitivity # In Linux, file names are case-sensitive.\nIn the example below, a series of distinct files stored in the same directory differ only by the case-sensitivity of their filenames.\n  Exclusively using lowercase characters generally makes thigs easier and less confusing to remember.\nUnicity within directories # File names must be unique within each directory.\nBearing in mind the case-sensitivity of filenames described above, all filenames within a given directory must be distinct.\nFor instance, using the \u0026gt; (greater than) symbol to redirect a stream to a filename that already exists will overwrite that file (i.e., it will not create another file with the same name).\nFor instance, the code below will either:\n Create the file file.txt if it does not exist (and add Hello as its contents). Overwrite the contents of the file if it already exists.  echo \u0026quot;Hello\u0026quot; \u0026gt; file.txt  Symbols allowed in filenames # Filenames can included a limited set of characters, including:\n Uppercase and lowercase letters. Digits. - (dash), _ (underscore).  Symbols to avoid in filenames # Some characters are not allowed or strongly discouraged in filenames, due to their special meaning in Linux.\nDo not use the space symbol in filenames. Instead, use the _ (underscore) or - (dash) symbols.\nFor instance, do not use my file.txt; use my_file.txt instead.\nðŸ‘‰  Quoting filenames that contain space can sometimes help the shell to recognise the full filename (e.g., 'my file.txt'). However, it is just best to avoid the issue altogether.   Avoid using the following special symbols as well:\n % (percentage) $ (dollar sign) Â£ (pound) \u0026quot; (double quotation mark) ' (single quotation mark) / (forward slash) \\ (back slash) | (pipe) = (equal sign)  Readability # Use distinctive, human-readable names that give an indication of the content.\nFollow a consistent pattern that is both user-friendly to read and machine-friendly to process (e.g. sample1-replicate1-read1.fastq.gz).\nMake use of suffixes to identify file formats (e.g., .txt, .csv, .sh).\nDirectory structure # Put each project in its own directory, named after the project.\nOrganise files into directory structures that follow a consistent pattern.\nFor instance:\nmy_project/ â”œâ”€â”€ data/ â”‚ â”œâ”€â”€ fastq/ â”‚ â”‚ â”œâ”€â”€ sample1.fastq.gz â”‚ â”‚ â””â”€â”€ sample2.fastq.gz â”‚ â””â”€â”€ annotations/ â”‚ â”œâ”€â”€ genome.gtf.gz â”‚ â””â”€â”€ sample_metadata.csv â”œâ”€â”€ code/ â”‚ â”œâ”€â”€ scripts/ â”‚ â”‚ â”œâ”€â”€ hisat2.sh â”‚ â”‚ â””â”€â”€ featurecounts.sh â”‚ â””â”€â”€ notebooks/ â”‚ â”œâ”€â”€ differential_expression.R â”‚ â””â”€â”€ pathway_analysis.R â”œâ”€â”€ results/ â”‚ â”œâ”€â”€ sample1.bam â”‚ â”œâ”€â”€ sample2.bam â”‚ â””â”€â”€ read_counts.tsv â””â”€â”€ README.txt  Sub-directories are commonly created for:\n Raw sequencing data (e.g., FASTQ files). Publicly available data sets (e.g., Gene Expression Omnibus - NCBI). Reference genome, index, and annotations (e.g., Ensembl FTP). Analysis code (e.g., scripts, notebooks, pipelines). Analysis output files (e.g., tables, plots, reports).  "}).add({id:56,href:"/Help/docs/data-management/organise-and-backup/backup/",title:"Back up your work",description:"A brief introduction to best practices in backing up files.",content:"Backups and archives # What is a backup? # A backup is a copy of important data that is stored in an alternative location, so that it can be recovered the original data is deleted or becomes corrupted.\nWhat is an archive? # An archive is a copy of a completed project that has been stored on a remote computer for long-term storage with limited access.\nComputer systems used to archive data often have reduced cost for disk usage over long periods of time, but longer access time to retrieve data.\nSeveral commercial third-party services are available to archive large amounts of data (e.g., Amazon Glacier).\nðŸ‘‰  Before using third-party systems, check with funders if they allow data to be hosted externally.   Several public data repositories store research data at no cost, including:\n European Nucleotide Archive (ENA) - EMBL-EBI ArrayExpress - EMBL-EBI Gene Expression Omnibus (GEO) - NCBI Short Read Archive (SRA) - NCBI  ðŸ‘‰  Archived data should be carefully organised and annotated with comprehensive metadata for traceability and discoverability.   Where should a backup be made? # A backup copy should be made a computer that is different and preferably physically distant from the computer where the original data files exist.\nA different computer ensures the survival of the data in the event of a failure affecting specifically the computer where the original data is stored.\nA distant location ensure the survival of the data in the event of destructive events affecting all computers in the location of the computer where the original data is stored.\nWhich files should be backed up? # Experimental raw data # Experimental raw data include files that contain data collected directly from the experiment and irrecoverable if they were lost.\nFor instance, FASTQ files are generally considered the raw data files for sequencing experiments (sometimes, BCL files are returned to clients by sequencing facilities, and require conversion to FASTQ files prior to downstream analyses).\nAll raw data files that cannot be regenerated in the event of loss must are a top priority to backup.\nAnalysis code # All bespoke code written and executed for a particular project should be backed up as soon as possible after \u0026ndash; or while \u0026ndash; executing it.\nCode typically includes scripts and notebooks executed on the CCB cluster in a Terminal application or integrated development environment (e.g., RStudio Server, jupyter), as well as code executed in a similar way on personal computers (e.g., RStudio Desktop).\nIn particular, all code that is necessary to transform raw data into output files is a top priority to back up.\nWe recommend creating a separate GitHub repository for each project, and using that repository to maintain an up-to-date backup copy of each script and notebook for the associated project.\nEnvironment specifications # The version of programs used in scripts and notebooks should be recorded and backed up.\nWe describe the use of environment modules in the section Using environment modules. When using environment modules, we recommend writing scripts that explicitly load the version of the module that you wish to use, and back up those scripts.\nWe also recommend the use of package management systems described in the following sections:\n Conda.  Package managers can be used to automate the process of installing, upgrading, configuring, and removing computer programs in a consistent manner.\nIn addition, package managers generally include functionality to export the list of programs installed in an environment as a text file detailing the name and version of each program installed in the environment.\nWhich files should NOT be backed up? # Publicly available data sets # Data sets obtained from stable public websites (e.g., NCBI GEO, Ensembl FTP Download, UCSC Genome Downloads) do not need to be backed up.\nInstead, the source of those data sets should be recorded, so that they may be downloaded again if lost or corrupted.\nBetter yet, the commands executed to download those data sets should be saved in a script or notebook that is backed up with the analysis code (see above, Analysis code).\nShared files # Sometimes, large files re-used across many projects (e.g., reference genome sequences and annotations, some publicly available data sets) are downloaded by system administrators and stored in a shared location accessible by all users of the CCB cluster.\nThose shared resources should not be backed up by individual users.\nHowever, users should make sure that they are fully familiar with any pre-processing that may have been applied to those files (if any), and keep a record for the accurate reporting of their materials and methods.\nOutput files # Output files are the result of running programs \u0026ndash; through scripts and notebooks \u0026ndash; on files that contain experimental raw data.\nOutput files do not need to be backed up, as they can be regenerated from the experimental raw data and the analysis code that were backed up (see above, Which files should be backed up?).\nWith that said, it can be beneficial to make copies of some key output files that are commonly used in downstream analyses and would take a long time to regenerate (e.g., RNA-seq count matrix).\nðŸ‘‰  A backup of key output files does not contribute toward reproducibility, but can provide a continuity plan in the event of lost or corrupted files, while those files are being regenerated from backed up raw data, scripts and notebooks.   How often should data be backed up? # Data should be backed up at an interval determined by how often the data changes, how valuable it is, and how long it takes to perform the backup.\nHow many backup copies should I make? # One backup copy on a reliable computer is often sufficient.\nHowever, it is a good idea to back up key data in more than one place, if possible.\nHow do I perform a backup? # The first thing to know is that backups are not run automatically on the CCB cluster. Users are responsible for performing the backup of their own data.\nThe rsync command is one of the handiest and most reliable backup tools available on the CCB cluster.\nThe rsync program is a utility for efficiently transferring and synchronizing files between a computer and a storage drive and across networked computers by comparing the modification times and sizes of files. In other words, calls to the rsync command will only transfer files that were modified since the previous backup, saving valuable time during subsequent backups.\nA typical backup using the rsync command looks as follows:\nrsync -avzh /directory/to/backup username@remote:/backup/directory  In particular:\n The option -a enables the archive mode enabling all of the following options:  -r \u0026ndash; Recursively backup the target directory. -l \u0026ndash; Copy symlinks as symlinks (i.e., not the target files themselves). -p \u0026ndash; Preserve permissions. -t \u0026ndash; Preserve modification times. -g \u0026ndash; Preserve group. -o \u0026ndash; Preserve owner. -D \u0026ndash; Preserve device files special files.   The option -v make the command verbose (i.e., prints informative messages as it runs). The option -z compresses files as it is transferred, reducing the amount of data being transmitted. The option -h outputs numbers in a more human-readable format (i.e., with units). The first positional argument represents the path to the directory that you wish to back up. The second positional argument describes the path to the directory on the remote computer where you wish to make a backup copy of the original data:  If you use the command above as template:\n Replace /directory/to/backup by the path to the directory that you wish to back up. Replace username by your username on the remote computer. Replace remote by the URL of the remote computer. Replace /backup/directory by the path to the directory in which you want to create or update the backup.  "}).add({id:57,href:"/Help/docs/data-management/organise-and-backup/compress/",title:"Compress large files",description:"A brief introduction to best practices in compressing large files.",content:"Why compress files? # Disk space is a resource that even the largest High Performance Computing (HPC) clusters can run out of.\nIt is every user\u0026rsquo;s responsibility to manage their data files and ensure that they do not use excessive disk space unnecessarily.\nMany programs automatically produce output files in compressed formats, to minimise their footprint on the disk space. When that is not the case, users are encouraged to compress large files themselves.\nWhich files should be compressed? # A rule of thumb is to compress all non-trivial text files greater than 1 MB in size.\nThe ls -lh \u0026lt;directory\u0026gt; command can be used to list files in \u0026lt;directory\u0026gt;, along with their file size in human-readable format (i.e., with units).\nHow to compress files? # We describe commands to compress \u0026ndash; and decompress \u0026ndash; files in the page Compress and decompress files.\nBriefly:\n The gzip command can be used to compress individual files. The tar command can be used to combine multiple files into a single archive file.  Working with compressed files # Some programs do not work with compressed files.\nIf you need to decompress a file for downstream processing, re-compress it afterwards.\nFor instance:\ngunzip file.fastq.gz # de-compress cat file.fastq # use the file gzip file.fastq # re-compress  In some cases, commands can be written to stream decompressed data directly from compressed files on the fly, without the need to make a decompressed copy of the file.\nFor instance (Credits):\nsickle se -f \u0026lt;(gunzip -c file.fastq.gz) -t sanger -o trimmed.fa  In particular:\n The sickle program expects a decompressed FASTQ file as input to the option -f. The syntax \u0026lt;(gunzip -c file.fastq.gz) runs the command gunzip -c file.fastq.gz in a sub-shell and passes the output of the command to the option -f as a temporary file. The compressed file file.fastq.gz is left unchanged during the entire process. This method is called Process substitution. It can be used in regular Linux commands, not only bioinformatics programs!  "}).add({id:58,href:"/Help/docs/data-management/organise-and-backup/delete/",title:"Delete intermediate files",description:"A brief introduction to best practices in deleting intermediate files.",content:"Which files can be deleted? # Many bioinformatics workflows involve multiple steps of processing on certain files (e.g., filtering, sorting, annotation).\nThose steps generate and store multiple copies of those files at the various steps of processing.\nIn such cases, you should consider deleting all but the final copy of that file.\nFor instance:\n# Run HISAT2 to map reads hisat2 -f \\ -x $HISAT2_HOME/example/index/22_20-21M_snp \\ -U $HISAT2_HOME/example/reads/reads_1.fa \\ -S example1.sam # Run samtools view to convert from SAM to BAM samtools view -bS example1.sam \u0026gt; example1.bam # Run samtools sort to sort mapped reads samtools sort example1.bam -o example1.sorted.bam # Remove intermediate files rm example1.sam example1.bam  In particular:\n The rm command is used to delete all files except for the final example1.sorted.bam.  ðŸ‘‰  You should keep intermediate files as long as necessary for debugging purposes. only delete intermediate files when you are sure that you will not need them anymore.   Which files should not be deleted? # Small files # It is usually unecessary to delete small intermediate files.\nThe benefit of disk space recovered is usually negligible against the time and effort necessary to regenerate those files if necessary.\n"}).add({id:59,href:"/Help/docs/data-management/organise-and-backup/symbolic-links/",title:"Use symbolic links",description:"A brief introduction to best practices in data management using symbolic links.",content:"What are symbolic links? # We describe symbolic links in the section Link to files and directories.\nBriefly, a symbolic link is a shortcuts to target file or directory.\nThe link itself is materialised as a very small file that only indicates the location of the target file.\nAs such:\n The size of the target file does not impact in any way the size of the symbolic link. A symbolic link will always be smaller than the target data file, or a copy of that file (unless the target file is empty).  Why use symbolic links? # A symbolic links make the target file accessible from a different location (i.e., directory) without the need to make a copy of the file, greatly reducing disk usage.\nHow do I create symbolic links? # We describe the creation of symbolic links in the section Creating soft links.\nBriefly:\nln -s target_file.txt link_file.txt  How to use symbolic links # Symbolic links can be used in place of the original target file.\nOften, symbolic links stored in the working directory offer a shorter alternative to longer file paths to target files located elsewhere in the filesystem.\nFor instance:\nln -s /path/to/target/file.txt link.txt cat /path/to/target/file.txt cat link.txt  In particular:\n The ln -s command is used to create a symbolic link. The two cat commands are entirely equivalent; the first command directly accesses the original file, while the second command follows the symbolic link ultimately accessing the same target file.  "}).add({id:60,href:"/Help/docs/data-management/organise-and-backup/quota/",title:"Monitor your disk space quota",description:"A brief introduction to best practices in data management using disk space quotas.",content:"What is disk space quota? # Briefly, disk space quota are limits on the amount of disk space that individual users can use on a shared computing system.\nOnce a user exceeds their allowed disk space quota \u0026ndash; possibly after a set grace period \u0026ndash; they will not be able to create or edit any file in the filesystem until they either:\n Delete or compress files, to return within their allowed quota (where possible). Request an increase to their quota.  Why monitor disk usage? # CCB accounts are subject to charging as a result of the costs associated with the necessary storage and computing facilities.\nTo ensure fair usage of the shared disk resources, the CCB cluster enforces quotas that restrict the amount of disk space that users may utilise to store files for their various projects.\nUsers who exceed their quota may see some of their commands fail on the CCB cluster, in particular commands that attempt to create files.\nMonitoring disk space usage regularly allows users to anticipate times when they may approach or exceed their allowed quota, and take preventive action, including:\n Delete unused or intermediate output files. Compress large files. Request an increase to their quota.  Users are encouraged to regularly check their disk usage and quota, to manage their files and remain within their disk usage quota or request an increase to their quota (at an additional cost). Refer to the Help section for ways to contact system administrators and request increase to your quotas.\nHow to monitor disk usage? # The du command can be used to estimate file space usage.\nThe option -h is commonly used to display values in human-readable form (i.e., with units).\nBy default, the du command reports the size of each directory in the working directory.\nFor instance:\ndu -h    The option -s is commonly used to summarise the total disk space usage across all the contents of the directory.\nMoreover, the du command can be given the path to a target directory, instead of the working directory.\nFor instance:\ndu -hs ~/ccb_demo    How to monitor your quota usage? # The quota command can be used to query your current quota usage and total allowance, across all directories, files, and projects.\nThe option -s displays values in human-readable format (i.e, with units).\nquota -s    "}).add({id:61,href:"/Help/docs/data-management/transfer/",title:"File transfers",description:"File transfers doks.",content:""}).add({id:62,href:"/Help/docs/data-management/transfer/filezilla/",title:"Transfer files using FileZilla",description:"A brief introduction to transferring files using FileZilla.",content:"What is FileZilla? # The FileZilla Client Client is a free solution for transferring files between computers via FTP/SFTP/FTPS protocols.\nMost commonly, the FileZilla Client is used to transfer files between a local computer \u0026ndash; where the FileZilla Client is installed \u0026ndash; and a remote computer (e.g., the CCB cluster).\nWhile connected to a remote computer, the main window of the FileZilla typically looks as follows:\n  Create site profiles # We encourage users to create a profile for each remote computer that they regularly connect to using FileZilla.\nA profile stores all the information necessary to connect to a remote computer. When a profile is correctly set up, users can select a profile and click the Connect button \u0026ndash; instead of the Quickconnect button associated with the fields Host:, Username:, Password: and Port: \u0026ndash; saving time and avoiding typographical mistakes.\nThe FileZilla Site Manager can be open from the menu as follows:\n Click of File. In the menu, click on Site Manager....    In particular:\n Click the button New site to initialise a new profile. Type a short yet descriptive name for the profile. In the field Protocol:, choose SFTP - SSH File Transfer Protocol. In the field Host:, type the URL of one of the login nodes (see General information). You may leave the field Port: empty. In the field Logon Type:  Choose Key file if you have created an SSH Key pair (see Set up an SSH key pair).   In the field User:, type your username on the CCB cluster. In the field Key file:, select the private SSH key that you created on your personal computer (typically located at ~/ssh/id_rsa).  You may be prompted to convert your private key file to a format supported by FileZilla. If so, click Yes and follow the one-time instructions in the next section below. At the end of the process, the converted key should be automatically selected. When setting up subsequent profiles, select the converted key directly.   Click OK to save the profile (this usually closes the Site Manager). Reopen the Site Manager, select the profile, and click Connect.  Convert a private SSH key file # The private SSH key file generated using the ssh-keygen command is not directly supported by the FileZilla client.\nWhen selecting a private key file generated using the ssh-keygen command, the FileZilla Client may prompt you to convert the key file to a format supported by the FileZilla Client.\n  Click Yes.\nIf your private key file is protected by a passphrase, you may be prompted to type that passphrase, to temporarily unlock the private key file during the conversion process, and protect the converted key file with the same passphrase.\n  When prompted to select a filename for the converted key file:\n In the field Save As:, type a descriptive filename (e.g., id_rsa.filezilla). In the field Where:, navigate your local filesystem and select an appropriate location (e.g., ~/.ssh). Click the Save button.    Connect to a remote computer # Open the Site Manager of the FileZilla Client as follows:\n Click of File. In the menu, click on Site Manager....    ðŸ‘‰  A keyboard shortcut specific to your operating system may be available to open the Site Manager more rapidly (e.g., on macOS, press the 'Command' and 'S' keys simultaneously).   In the Site Manager:\n Select the profile that you wish to use. Click the Connect button.  Status messages in the panel at the top of the FileZilla Client should display information reporting successes and failures during the connection.\nWhen succesfully connected, the panel on the right of the FileZilla Client should display a file explorer view of the filesystem on the remote computer.\nThe panel on the left of the FileZilla Client should continue to display a file explorer view of the filesystem on the local computer.\n  Navigate filesystems # While connected to a remote computer, users can use the two file explorers to navigate the local and remote filesystems, respectively.\nEach file explorer panel \u0026ndash; local and remote \u0026ndash; are sub-divided in two panels:\n The top panel displays the filesystem as a hierarchical tree, where directories can be expanded and collapsed, to show and hide their contents, respectively. The bottom panel displays detailed information about the contents of the directory currently selected in the top panel.  Both panels \u0026ndash; top and bottom \u0026ndash; can be used to navigate the corresponding filesystem.\nTransfer files # While connected to a remote computer, users can transfer files and directories between the local and remote computer using drag-and-drop between the left and right panels (in either direction).\nWhen using drag-and-drop, make sure to release the click button only when the cursor is an area that corresponds exactly to the directory in which you want to transfer the data.\nConsider the example below.\n   Dropping an item from the left into the red area on the right would copy it into the parent directory of the currently active directory on the right (i.e, next to the active directory on the right). Dropping an item from the left into the green area would copy it into the directory dir1. Dropping an item from the left into the blue area would copy it into the currently active directory on the right (in this case, ~/ccb_demo).  Move files between remote directories # While we recommend using the command line to move files within the remote filesystem, it is possible to drag-and-drop files within the panel that displays the file explorer on the remote computer (i.e., the panel on the right).\nImportantly, using drag-and-drop within a panel does not make a copy of the item, but moves it from one location to another.\n"}).add({id:63,href:"/Help/docs/data-management/transfer/download/",title:"Download publicly available files",description:"A brief introduction to downloading publicly available files.",content:"Public data repositories # Public data repositories provide platforms for distributing commonly used reference data sets (e.g., reference genomes, gene annotations), as well as experimental data sets associated with published work.\nFor instance:\n FTP Download - Ensembl â†’ ArrayExpress - EMBL-EBI â†’ ENA Browser - European Nucleotide Archive - EMBL-EBI â†’ Gene Expression Omnibus - NCBI â†’ Home - SRA - NCBI â†’  It is often necessary to download data files from those public repositories onto the CCB cluster before they can be used by programs running on the CCB cluster.\nA number of Linux commands are available to download remote files.\nwget # The wget command is a free software package for retrieving files using HTTP, HTTPS, FTP and FTPS; the most widely used Internet protocols.\nFor instance:\nwget ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz  By default, the wget command downloads files in the working directory. We recommend using the cd command to set the working directory before using the wget command.\nAlternatively, the -P option can be used to set a directory prefix where all files and subdirectories will be saved.\nFor instance:\nwget \\ -P ~/ccb_demo \\ ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz  rsync # We describe the rsync command in the section How do I perform a backup?.\nWhile the rsync command is more commonly used to perform and retrieve backups, its synchronisation functionality can also be used to download files and directories.\ncurl # The curl command can be used to download individual files.\nFor instance:\ncurl \\ -o ~/ccb_demo/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz \\ ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz  In particular:\n The option -o can be used to declare the location and filename of the downloaded file.  Alternatively, the options -O -J can be used together, to automatically identify the filename of the original file on the remote computer, and save the file locally with the same name. However, in that case, the file can only be downloaded in the working directory. We recommend using the cd command to set the working directory before using those options.\nFor instance:\ncd ~/ccb_demo curl \\ -O -J \\ ftp://ftp.ensembl.org/pub/release-102/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.primary_assembly.fa.gz  In particular:\n -O save the file locally using the basename of the original file () -J ensures that -O respects the basename of the original file on the remote computer, instead of extracting a basename from the URL (useful when the two are different).  Verify integrity of downloads # Principle # On occasions, files can be corrupted during their transfer.\nSeveral commands \u0026ndash; including md5sum, sha256sum, and sum \u0026ndash; can be used to rapidly verify whether downloaded files match their original copy on the remote computer.\nFor those commands to be of any use, the remote provider of the downloaded files must have run the command themselves on their side, and made available the output of the command for each file of interest (as well as instructions to properly use those outputs).\nIf that is the case, users can then run the same command on their own copy of the downloaded files, and compare the output with that of the provider.\nManual comparison # In some cases, providers display the output of the command on their website.\nFor instance:\n  In those case, users should run the command on the downloaded file, and manually compare the output of the command to the value displayed on the website for the corresponding file.\nFor instance:\nsha256sum Miniconda3-latest-Linux-x86_64.sh    At that point, rather than visually comparing each character between the two outputs, we recommend copying the output of the command in the Terminal and searching for that string of characters on the web page. If the two strings match, most web browser will highlight the corresponding value on the web page.\n  Check MD5 sums in a file # In some cases, providers make the output of the md5sum command available as a file that can be downloaded itself.\nThe contents of that file typically looks as shown below:\n  In particular:\n The first column displays the MD5 sum for a specific file. The second column display the path to that file (relative to the working directory where the md5sum command was run from).  Users in possession of that file can give it to the options -c of the md5sum command. The md5sum command will then run for each file listed in the file, and automatically compare the value obtained for the downloaded file to the reference value listed in the file (corresponding to the md5sum command run on the original file on the remote computer).\nFor instance:\nmd5sum -c md5.out    "}).add({id:64,href:"/Help/docs/slurm/hpc-resources/",title:"HPC resources",description:"General information about the resources available through the Slurm workload management system on the CCB cluster.",content:"Introduction # The CCB cluster is a high-performance computing system shared by many users who run programs using a pool of computational resources to process data simultaneously.\nHowever, the resources that may be used at any point in time are limited by the total amount of resources available on the CCB cluster.\nTo ensure fair access to all users, the CCB cluster uses the Slurm workload manager. Users are required to submit their work as job scripts that describe the commands they wish to execute, and the amount of resources that they wish to grant to those commands during their execution. In turn, Slurm controls the execution of submitted jobs when sufficient resources are available, and manages a queue of pending work while waiting for sufficient resources to become available.\nResources # The table below summarises the types of compute nodes available.\n    Nodes Cores (per node) RAM (per node)     Large nodes 1 40 900GB   Batch nodes 20 24 240GB    ðŸ‘‰  In addition, the cluster contains 4 x NVIDIA Titan RTX cards for parallel and multi-core tasks such as deep learning.   The full list of compute nodes and associated information can be obtained as follows.\nsinfo --Node --long    More information about individual compute nodes can be obtained using commands detailed in the page Slurm commands.\n"}).add({id:65,href:"/Help/docs/slurm/commands/",title:"Slurm commands",description:"A cheatsheet of Slurm commands.",content:"Motivation # Once logged into the CCB cluster, a number of Slurm commands can be used to query resources available and the current workload.\nIn this page, we provide a cheatsheet of the most commonly used commands along with a brief description of their purpose and advice on best practices.\nPartitions # Summary information about partitions (i.e., individual job queues) can be accessed as follows.\nsinfo    The sinfo command displays the name of partitions in the first column, and additional information in a customisable set of additional columns. The full list of fields and their description can be found in the Slurm documentation â†’.\nIn the example above:\n The asterisk symbol * indicates the default partition, to which jobs are submitted when users don\u0026rsquo;t request a specific queue. The field AVAIL uses the value up to indicates partition that are accepting new jobs. The field TIMELIMIT indicates the maximum time limit allowed for jobs submitted to each partition (in the format hours:minutes:seconds). The value infinite is used to identify partitions without a job time limit. The field NODES indicates the number of nodes with each particular configuration in each partition. The field STATE indicates the state of nodes in each partition; for instance:  The value alloc indicates nodes allocated to one or more jobs. The value idle indicates nodes that are not allocated to any job. The value mixed indicates nodes where some CPUs are allocated a job while others are idle. The value drain indicates nodes that are currently executing jobs, but do not accept new jobs, per administrator request (e.g., for maintenance).    Nodes # The full list of individual nodes and associated information can be obtained as follows.\nsinfo --Node --long    In particular:\n The --Node option reports a separate line for each node in each partition (a node can be shared by multiple partitions). The --long option reports more detailed information.  More detailed information about the configuration of each individual nodes can be obtained as follows.\nscontrol show node  The full list of nodes above may be excessive. Information about a particular node can be obtained as follows.\nscontrol show node \u0026lt;nodename\u0026gt;  Replace \u0026lt;nodename\u0026gt; by the name of the particular node.\n"}).add({id:66,href:"/Help/docs/slurm/links/",title:"Links to Slurm documentation",description:"Links to external resources about Slurm.",content:" Slurm Documentation Slurm Quick Start User Guide WIMM CCB Computing Infrastructure  "}).add({id:67,href:"/Help/docs/conda/introduction/",title:"Introduction to Conda",description:"General information about the Conda package management system.",content:"Why use Conda? # The CCB cluster provides many bioinformatics software packages through the Environment Modules system, managed by system administrators.\nWhen software is not available through the module system, users may use the Conda package management system to install software packages themselves, and even organise distinct environments that may contain different version of those software package for different projects.\nAdditionally, when installing a package, Conda identifies and resolves package dependencies, installing the latest appropriate version of every dependency for the target package.\nGet started # In the following pages, we provide guidance to install and setup the Conda package management system for an individual user on the CCB cluster, we demonstrate how to install packages and create distinct environments, and how to activate environments before using the newly installed software packages on the command line.\n"}).add({id:68,href:"/Help/docs/conda/installation/",title:"Install Miniconda",description:"Installing the Conda package management system.",content:"Download the installer # In a new Terminal application, log into the CCB cluster.\nDownload the latest Miniconda installer for Linux using the following command.\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh  For reference, the URL to the installer file in the command above was obtained from the Latest Miniconda installer links.\nRun the installer # Use bash to execute the installer file as demonstrated below.\nbash Miniconda3-latest-Linux-x86_64.sh -b -p ~/conda  Note the use of options to control the behaviour of the installer:\n The option -b runs the installer in \u0026lsquo;batch\u0026rsquo; mode (without manual intervention). This assumes that you agree to the license agreement. It also prevents the installer from modifying your ~/.bashrc file. The option -p specifies the installation path, the location where Conda will be installed. We recommend specifying ~/conda, which will create a directory conda in your home directory.  ðŸ‘‰  Some of the instructions below rely on this specific installation path. If you decide to change it, be sure to adapt the following instructions accordingly.   Once the installation successfully completes, you may want to delete the installer file.\nrm Miniconda3-latest-Linux-x86_64.sh  Test the installation # Before going further, we recommend testing that the new installation of Conda works as expected.\nsource ~/conda/etc/profile.d/conda.sh conda activate base    First, the source command executes a script that makes Conda discoverable in the Bash environment of your Terminal application. This command is not expected to return any information in the Terminal.\nThen, the conda command is used to activate the base environment that was created during the installation process. Again, this command is not expected to return any information in the Terminal. However, after that command completes, the prefix (base) should be visible at the start of the prompt.\nThe which command can also be used to verify that the conda command is found on the PATH, at the expected location.\nwhich conda    Deactivate the environment # Before going further, we recommend deactivating the base environment.\nTo do so, the conda command is used with the deactivate sub-command.\nconda deactivate  ðŸ‘‰  Deactivating at this point will allow us to test a new shortcut that we set up in the next section.   Create a shortcut # Open your ~/.bashrc file and add the following lines.\n# Create an alias to activate the Conda base environment alias conda_activate_base='source ~/conda/etc/profile.d/conda.sh \u0026amp;\u0026amp; conda activate base'  Test the shortcut # Run the following lines of code sequentially.\nsource ~/.bashrc conda_activate_base    First, the source command is used to execute the commands in ~/.bashrc file, including the alias command that defines the new shortcut.\nThen, the newly defined alias is used to invoke the command that is now assigned to it. This shortcut should have the same effect as typing the full command defined in the alias, activating the base Conda environment and adding the (base) prefix to the prompt.\nNext steps # From this point, new instances of the Terminal application will automatically execute the ~/.bashrc file and define the alias as soon as you connect to the CCB cluster.\nAs a result, when you connect to the CCB cluster in new instances of the Terminal application, you will only need to call the alias by name.\nconda_activate_base    Furthermore, you should only call the alias once per Terminal application, as there is no benefit to activate the base environment multiple times per session.\nThe only exception being if you deactivate the base environment and wish to re-activate it. For instance, notice the (base) prefix appearing and disappearing from the promp as the environment is activated and de-activated in the example below:\nconda_activate_base conda deactivate conda_activate_base    "}).add({id:69,href:"/Help/docs/conda/first-steps/",title:"First steps with Conda",description:"Configuring Conda and installing mamba.",content:"Pre-requisite # Activate the base Conda environment, as demonstrated in the page Install Miniconda.\nHelp and information # The conda --help command can be used to list the main sub-commands of the conda executable, along with a brief description of their purpose.\nconda --help    The conda info command can be used to display information about your current installation.\nconda info    ðŸ‘‰  You should always include the output of the 'conda info' command with all your bug reports, as that information provides important context for your own situation.   Configuration # Conda packages are stored in different locations called channels. More information is available in the Conda documentation â†’.\nConfigured channels are searched in a specific order when installing packages, so it is important to configure which channels are searched and in which order.\nWe recommend the following channels, in decreasing order of priority:\n bioconda conda-forge default  In other words, Conda will attempt to install packages from the bioconda channel first, and then sequentially search the channels conda-forge and defaults when packages cannot be found in earlier channels in the priority list.\nAdding new channels automatically moves them to the top of the priority list. With that in mind, the commands below sequentially add channels from the lowest priority channel to the highest priority one, to produce the desired order of priority described above.\nconda config --add channels defaults conda config --add channels conda-forge conda config --add channels bioconda  The conda info command (described earier on this page) can be used to verify the list of channels that are currently configured, among other pieces of information.\n  List installed packages # The conda list command can be used to list the packages installed in the current environment.\nconda list    ðŸ‘‰  The screenshot above represents an example output reporting package version numbers at a certain point in time. It is very likely and entirely normal that your own package version numbers differ from those.   Update installed packages # The conda update command can be used to updates conda packages (in the current environment) to the latest compatible version.\nThe --all option checks all packages in the environment (rather than explicitly specifying individual packages).\nconda update --all  In particular, the command will produce a detailed plan, listing packages that may be downloaded and updated in the Conda environment.\nWe recommend careful reviewing the information detailed in the package update plan before agreeing to proceed, by typing y and pressing the Return key when prompted.\n  Update Conda itself # Part of the output of the conda update command may report whether a newer version of the conda package itself is available, alongside instructions to update that package.\n  Follow those instructions when you are ready to update your Conda installation.\nconda update -n base -c defaults conda  ðŸ‘‰  In the example above, the option '-c' is used to override the list of channels used during this particular command, ensuring that the 'conda' package is obtained safely and exclusively from the channel 'defaults'.   Search available packages # The conda search command can be used to search for packages available in any of the configured channels.\nFor instance, the example below demonstrates how to search for a Conda package called mamba. Note that the list of results was rather long and only the first results are shown in the screenshot.\nconda search mamba    The output of the conda search command includes key information for identifying the list of versions available and potentially selecting a specific build to install. For instance, the Channel column indicates the channel in which each package build is stored, which can be especially important for packages available from different channels.\nVerify that a package is not installed # The conda list \u0026lt;package\u0026gt; command can be used with the name of a specific package, to list any version of that package installed in the environment.\nThe output will be presented as a table, and the table will be empty (i.e., only column headers) if the package is not currently installed in the environment.\nconda list mamba    Alternatively, the Bash command which can be used to check whether an executable known to be present in a Conda package is detected on the PATH.\nwhich mamba    Install a package # The conda install command can be used to install packages.\nHowever, the built-in Conda installer can be quite slow to identify and resolve dependencies between packages, which led to a re-implementation of the conda package manager in C++, distributed in the mamba package.\nInstall the mamba package using the command below. Make sure to type y and press the Return key when prompted to proceed with the package plan.\nconda install mamba    Verify that a package is installed # In the example below, the conda list \u0026lt;package\u0026gt; command is used to list installed packages that contain the pattern mamba in their name.\nconda list mamba    In the example below, the Bash command which is used to display the location of the executable mamba that is now detected on the PATH.\nwhich mamba    Alternatively, the mamba executable itself can be invoked, for instance with the --help option, to display the help page of the program, implictly verifying that the executable itself is installed and working properly.\nmamba --help    Using Mamba # Mamba is implemented as a drop-in replacement for Conda.\nIn other words, the mamba executable can be used in many cases as a replacement for the conda executable, to produce the same result much more rapidly.\nFor instances:\nmamba info mamba update --all mamba search \u0026lt;package\u0026gt; mamba install \u0026lt;package\u0026gt;  Replace \u0026lt;package\u0026gt; by the actual name of any package of interest to you.\n"}).add({id:70,href:"/Help/docs/conda/environments/",title:"Use Conda environments",description:"Creating and managing Conda environments.",content:"Motivation # Different projects often require different versions of the same program, or even mutually incompatible versions of different programs.\nConda provides the possibility to create and manage distinct environments, each containing a set of software packages that is independent from all other Conda environments on the same computer.\nUpon initial installation (see section Install Miniconda), the Conda installer automatically creates an environment called base. We recommend keeping the base environment as minimal as possible, and creating new Conda environments for testing and working as required by individual projects. A notable exception is the mamba package that we recommend installing directly in the base environment (see section First steps with Conda).\nðŸ‘‰  This section demonstrates the creation and management of Conda environments using a number of demonstration environments. Feel free to remove all those environments at the end of this section.   Get started # Remember to activate the base Conda environment.\nFor instance, using the alias defined on the page Install Miniconda:\nconda_activate_base  The conda env command includes a set of sub-commands to create and manage Conda environments.\nðŸ‘‰  In many cases, the 'conda' and 'mamba' commands can be used interchangeably, as Mamba is implemented as a faster drop-in replacement for Conda.   The conda env --help command can be used to display the help page and the list of available sub-commands.\nmamba env --help    List environments # The mamba env list command can be used to list existing Conda environments (for the current user).\nmamba env list    ðŸ‘‰  Conda environments are private to each user. Users cannot see each other's Conda environments.   Create an environment # The mamba create command can be used to create a new environment.\nmamba create -n ccb_demo_env    In particular:\n The option -n declares the name of the environment to create. This must be a name that is not used yet (use mamba env list to list existing environments).  At this point, the command mamba env list can be used to verify that the new environment was created.\nmamba env list    In particular:\n Each environment is listed on a separate line, indicating its name and the location of its directory in the filesystem. The * symbol indicates the environment that is currently active. Newly created environments are not automatically activated. In this example, the base environment is still active at this point.  Activate an environment # Conda environments do not take effect until they are activated.\nIn effect, a Conda environment is a directory that contains a specific collection of conda packages. Activating an environment loads all the packages that are installed in the directory associated with that environment. More information on Conda environments is available in the Conda documentation.\nThe conda activate \u0026lt;name\u0026gt; command can be used to activate an environment using the name of that environment.\nFor instance, the ccb_demo_env environment that we created above can be activated as follows:\nconda activate ccb_demo_env    Separately, the mamba list command can be used to verify that the new environment does not contain any installed package yet.\nmamba list    Install packages in an environment # The mamba install command can be used to install packages in the active environment exactly as in the base environment.\nFor instance, the most recent version of the python and r-base packages available can be installed as follows:\nmamba install python r-base    ðŸ‘‰  Due to the considerable number of dependencies that need to be installed, the screenshot above only displays the first lines of the package installation plan.   Remember to type Y and press the Return key when prompted to proceed with the package installation plan.\nThe mamba list command can be used to verify that the Conda environment now includes the requested packages, as well as their dependencies.\nmamba list    ðŸ‘‰  Due to the considerable number of packges installed in the environment, the screenshot above only displays the first lines of output.   The Bash command which can also be used to verify that the python and R executable files are now available from within the Conda environment.\nwhich python which R    ðŸ‘‰  Incidentally, the example screenshot above reveals that Conda created an alias for the 'R' command, which automatically adds the '--no-save' option. This forces R sessions to quit without saving the R workspace.   Create an environment from a list of packages # In the previous sections, we have demonstrated two separate steps:\n creating a new environment installing packages in that new environment  Those two steps can be combined into a single command.\nFor instance, a new environment named ccb_demo_env_2 can be created while immediately installing the latest version of the python and r-base packages as follows:\nmamba create -n ccb_demo_env_2 python r-base    Again, the conda activate command must be used to manually activate the new environment before it can be used. Then, the Bash command which can be used to verify that the two executable files are found in the new environment.\nconda activate ccb_demo_env_2 which python which R    Remove an environment # The conda remove command can be used to remove environments that are not needed anymore (e.g., test environment, completed project).\nImportantly, if the environment that you wish to remove is active, make sure to deactivate it first, using the command conda deactivate.\nThen, the mamba remove command can be used with a number of options. In particular:\n The option --name specifies the name of the environment to remove. The option --all indicates that all packages in that environment must be removed. This option is essential to indicate that the entire environment must be removed.  conda deactivate mamba remove --name ccb_demo_env_2 --all    The mamba env list command can be used to verify that the environment removed is indeed not listed anymore.\nmamba env list    ðŸ‘‰  We recommend practicing and getting comfortable with the creation and removal of environments. In doubt, it is often much easier and safer to remove an environment and start over.   Create an environment from a YAML file # Conda supports the the YAML file format for describing Conda environments, as well the desired channel priority list and environment name.\nFor instance:\nname: ccb_demo_yaml channels: - bioconda - conda-forge - defaults dependencies: - python - r-base  Create a file named ccb_demo.yaml, use copy and paste to fill it with the contents above, and save and close the file (for instance, use nano).\nnano ~/ccb_demo.yaml    ðŸ‘‰  The name of the file itself does not matter, but we recommend using the same name as the environment that it is used to create.   Then, the mamba install command can be used to create an environment using that file, as follows:\nmamba env create -f ~/ccb_demo.yaml    It is possible to override the name of the environment specified in the YAML using the option -n, to give a different name to the new environment.\nFor instance:\nmamba env create -n ccb_demo_from_yaml -f ~/ccb_demo.yaml  ðŸ‘‰  For traceability, we generally recommend using the name of the enviroment defined in the YAML file.   Again, the conda activate command can be used to activate the new environment.\nconda activate ccb_demo_yaml  Export an environment to a YAML file # The mamba env export command can be used to display the specifications of an environment (i.e., name, version, and build of packages installed in the environment). Those specifications are extremely valuable for reproducibility, as they can be exported to a YAML file that may be saved for the record, or shared and used to replicate the very same environment elsewhere.\nFor instance, the contents in YAML format can be viewed as follows:\nmamba env export -n ccb_demo_env    In particular:\n The option -n specifies the name of the environment to export.  The output of the command above can be redirected to a file using the \u0026gt; symbol, as follows:\nmamba env export -n ccb_demo_env \u0026gt; ~/ccb_demo_env.yaml  The exported file can then be used to re-create the environment elsewhere as demonstrated in section Create an environment from a YAML file.\nCleanup # Thank you for reading through this section. Using what you have learned, feel free to remove all demonstration environments before moving on to the next section.\nRemember:\n Use mamba env list to display the list of existing environments. Use mamba remove --name \u0026lt;environment_name\u0026gt; --all to remove an environment (replace \u0026lt;environment_name\u0026gt; by the name of the environment).  For instance:\nmamba env list mamba remove --name ccb_demo_env --all mamba remove --name ccb_demo_yaml --all  Final words # At this point, you have seen the essential commands to create and manage Conda environments on a daily basis.\nTo recapitulate:\n mamba env list lists the existing environments. mamba create -n \u0026lt;name\u0026gt; creates an environment called \u0026lt;name\u0026gt;. conda activate \u0026lt;name\u0026gt; activates the (existing) environment called \u0026lt;name\u0026gt;. mamba install \u0026lt;name1\u0026gt; \u0026lt;name2\u0026gt; installs the Conda packages \u0026lt;name1\u0026gt; \u0026lt;name2\u0026gt; in the active environment. conda deactivate deactivates the active environment. mamba remove --name \u0026lt;name\u0026gt; --all removes the environment called \u0026lt;name\u0026gt; and all its packages. mamba env export -n \u0026lt;name\u0026gt; exports the environment called \u0026lt;name\u0026gt; to a format that can be saved to a file and used to restore the environment elsewhere through mamba env create -f \u0026lt;file\u0026gt;.  Remember that those commands offer a number of additional options that are not detailed in this section. Refer to the help page of the individual commands for further information (Use conda \u0026lt;command\u0026gt; --help and mamba \u0026lt;command\u0026gt; --help).\nThis documentation includes a Conda cheatsheet that describes commands and options relevant to common scenarios faced by users in their daily work. Feel free to Contribute!\nFinally, this documentation also includes a number of links to External resources about Conda.\n"}).add({id:71,href:"/Help/docs/conda/cheatsheet/",title:"Conda cheatsheet",description:"A cheatsheet of Conda commands.",content:"Search available packages # The search command can be used to search for available packages and package versions programmatically.\nFor instance:\nmamba search pysam  The following websites can also be used to manually search packages available in individual channels:\n bioconda: https://anaconda.org/bioconda/repo/ conda-forge: https://conda-forge.org/feedstock-outputs/index.html  Install a specific package version # To install a specific version of a package, add the = symbol, followed by the specific version number.\nFor instance:\nmamba install pysam=0.19.1  Reminder: use the search command to identify the the list of versions available for a given package.\nmamba search pysam  ðŸ‘‰  The search returns package versions in alphanumeric order, meaning that the most recent versions will be present at the end of the output.   Dry-run # For several commands, the --dry-run option can be used to only display what would have been done, without actually doing anything. This can be particularly helpful to safely assess the impact of any command that is likely to significantly alter the environment.\nFor instance:\nmamba install pysam --dry-run  Remove packages # The remove command can be used to remove installed packages from the environment.\nFor instance:\nmamba remove samtools  "}).add({id:72,href:"/Help/docs/python/setup/ccb/",title:"Using Python on the CCB cluster",description:"Using Python on the CCB cluster.",content:"Overview # As well as the standard Python versions that you would normally expect, the CCB team additionally preinstalled hundreds of additional packages available for all to use. These are available via the python-cbrg module.\nBasic usage # If you just want to get up and running with set of commonly used bioinformatics packages curated by the CCB team, you can do so with a single command:\nmodule load python-cbrg    ðŸ‘‰  The Spyder IDE is included in the standard Python installations.   Request additional packages # If you need to use a package which is not already installed, please contact the CCB team via the address below before attempting to install a local copy. In many cases the CCB team can easily add it to the central installation.\nAdvanced usage # The setup of the python-cbrg module uses the following system.\nThe python-base module contains fixed, unchanging installations of the base language. This is for safety â€“- they cannot be accidentally overwritten causing unexpected changes of behaviour. The module python-cbrg contain separate package and library repositories for each version of Python. Because packages and library versions also change over time, we take a snapshot of the state on a monthly basis and then lock this to prevent changes causing unexpected behaviour. A single current version for each provides a continual rolling \u0026lsquo;head\u0026rsquo; where changes are applied. Loading the python-cbrg module will automatically pull in the latest stable base and all packages or libraries.\nFor instance:\nmodule load python-cbrg module list    However, if you want to use a different version of the base, you can do that by loading it manually first:\nmodule load python-base/3.6.10 module load python-cbrg module list    Simmilarly, if you want to use a different version of the libraries, for example because a recent update broke something you relied on, you can do that by loading it manually:\nmodule load python-cbrg/202104 module list    Getting help # You can email the CCB team using the email address genmail@molbiol.ox.ac.uk. Using this address ensures your email is logged and assigned a tracking number, and will go to all the core team, which means the appropriate person or people will be able to pick it up.\nCopyright # This text is copyright University of Oxford and MRC and may not be reproduced or redistributed without permission.\nAuthor # Duncan Tooke (duncan.tooke@imm.ox.ac.uk) and Kevin Rue-Albrecht (kevin.rue-albrecht@imm.ox.ac.uk).\n"}).add({id:73,href:"/Help/docs/python/setup/",title:"Setup",description:"Python setup doks.",content:""}).add({id:74,href:"/Help/docs/python/setup/macos/",title:"Setup Python on macOS",description:"Setting up Python on macOS.",content:"ðŸ‘‰  This section does not involve the CCB cluster at all. Instead, it provides a guide for installing a working Python environment on the macOS operating system.   Install Visual Studio Code (VSCode) # Download the installer file # In your web browser, navigate to https://code.visualstudio.com/download.\nClick on the button \u0026lsquo;Mac\u0026rsquo;, making sure that it matches the version of your operating system.\n  You may be redirected to another web page. However, the download should start automatically.\nUnpack and install the application # In Finder, navigate to the downloaded file. This should be a ZIP archive.\nDouble-click on the downloaded ZIP file to extract its contents. This should be a file called \u0026lsquo;Visual Studio Code.app\u0026rsquo;.\nDrag and drop the file \u0026lsquo;Visual Studio Code.app\u0026rsquo; into your \u0026lsquo;Applications\u0026rsquo; folder.\nYou may then delete the ZIP archive that you downloaded.\nInstall VSCode extensions # Launch VSCode.\nClick on the \u0026lsquo;Extension\u0026rsquo; tab on the left.\n Search for the extension named \u0026lsquo;Python\u0026rsquo; and install it.     Search for the extension named \u0026lsquo;Live Share Extension Pack\u0026rsquo; and install it.    Install Anaconda navigator # In your web browser, navigate to https://www.anaconda.com/.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nðŸ‘‰  The various screens of the installer programs are subject to change. As such, we provide guidance below for indicative purposes only.    Accept all defaults.  Test your installation #  In the Launchpad, find and launch \u0026lsquo;Anaconda-Navigator\u0026rsquo;.  If prompted about a new version of Anaconda Navigator available, click \u0026lsquo;Yes\u0026rsquo; to update.\n"}).add({id:75,href:"/Help/docs/python/setup/windows/",title:"Setup Python on Windows",description:"Setting up Python on Windows.",content:"ðŸ‘‰  This section does not involve the CCB cluster at all. Instead, it provides a guide for installing a working Python environment on the Windows operating system.   Motivation # Instructions on this page should be followed in order to produce the desired effects, namely:\n Visual Studio Code (VSCode) must be installed before Git Bash, so that VSCode may be specified as the default editor for Git. Git Bash must be installed before Anaconda Navigator, so that Anaconda Navigator may detect Git Bash on the system path. VSCode must be installed before Anaconda Navigator, so that VSCode may be launched from Anaconda Navigator.  Install Visual Studio Code # Download the installer file # In your web browser, navigate to https://code.visualstudio.com/download.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nInstall VSCode extensions # Launch VSCode.\nClick on the \u0026lsquo;Extension\u0026rsquo; tab on the left.\n Install \u0026lsquo;Python\u0026rsquo;.     Install \u0026lsquo;Live Share Extension Pack\u0026rsquo;.    Install Git Bash for Windows # Motivation # Microsoft Windows does not have a built-in Terminal emulator (The MS Windows Command Prompt cmd.exe is a command-line interpreter that does not include many of the commands needed to connect to the CCB cluster).\nThe primary function of the Git Bash for Windows program is to provide a Bash emulation used to run Git from the command line. Conveniently, in its effort to offer that functionality, the Terminal emulator of Git Bash for Windows also includes all the commands that you will need to connect to the CCB cluster.\nDownload the installer file # In your web browser, navigate to https://git-scm.com/downloads.\nIn the \u0026lsquo;Downloads\u0026rsquo; section, click on \u0026lsquo;Windows\u0026rsquo;.\n  On the next page, click on the first link \u0026lsquo;Click here to download the latest 32-bit version of Git for Windows\u0026rsquo;.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nðŸ‘‰  The various screens of the installer programs are subject to change. As such, we provide guidance below for indicative purposes only.   Notably:\n Scroll to the bottom of the license. Click \u0026lsquo;Next\u0026rsquo;. Leave the destination location for the installation to the default value. Click \u0026lsquo;Next\u0026rsquo;. Select \u0026lsquo;Add a Git Bash Profile to Windows Terminal\u0026rsquo; in addition to the components selected by default. Click \u0026lsquo;Next\u0026rsquo;. Leave the name of the Start Menu folder to be created to the default value. Click \u0026lsquo;Next\u0026rsquo;. Set the default editor for Git to a text editor program that is installed on your computer and that you are comfortable with. We recommend [Visual Studio Code][visual-studio-code]. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice letting Git decide the name for the initial branch in newly created repositories. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice adjusting your PATH environment. You want to be able to use Git from Git Bash, the Command Prompt and the Windows PowerShell as well as third-party software looking for Git in PATH (e.g., RStudio Desktop). Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using the bundled OpenSSH. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using the OpenSSL library. Click \u0026lsquo;Next\u0026rsquo;. Select \u0026lsquo;Checkout as-is, commit Unix-style line endings\u0026rsquo;. Click \u0026lsquo;Next\u0026rsquo;. Leave the default choice using MinTTY. Click \u0026lsquo;Next\u0026rsquo;. Leave the default behaviour of git pull to the default value. Click \u0026lsquo;Next\u0026rsquo;. Leave the credential manager to the default value. Click \u0026lsquo;Next\u0026rsquo;. Leave the extra options to their default choices. Click \u0026lsquo;Next\u0026rsquo;. Leave the experimental options unselected. Click \u0026lsquo;Install\u0026rsquo;.  When the installer successfully completes, click \u0026lsquo;Finish\u0026rsquo;.\nYou may be presented with the Release Notes of the program, which you may choose to read before you close them.\nTest your installation # Once the installer has completed successfully:\n Open the Windows Start Menu. In the Windows Start Menu find and launch \u0026lsquo;Git Bash\u0026rsquo;. In the Git Bash for Windows Terminal emulator, type git and press the Return key.  This should display a help message listing the main subcommands of the Git program.\nInstall Anaconda navigator # In your web browser, navigate to https://www.anaconda.com/.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nðŸ‘‰  The various screens of the installer programs are subject to change. As such, we provide guidance below for indicative purposes only.    Install for all users (needs administrator rights). Accept all defaults.  Test your installation #  Open the Windows Start Menu. In the Windows Start Menu find and launch \u0026lsquo;Anaconda Navigator (Anaconda3)\u0026rsquo;.  If prompted about a new version of Anaconda Navigator available, click \u0026lsquo;Yes\u0026rsquo; to update.\n"}).add({id:76,href:"/Help/docs/r/setup/ccb/",title:"Using R on the CCB cluster",description:"Using R on the CCB cluster.",content:"Overview # As well as the standard R versions that you would normally expect, the CCB team additionally preinstalled hundreds of additional packages available for all to use. These are available via the R-cbrg module.\nBasic usage # If you just want to get up and running with set of commonly used bioinformatics packages curated by the CCB team, you can do so with a single command:\nmodule load R-cbrg    ðŸ‘‰  The RStudio application is a separate module called 'rstudio'.   Request additional packages # If you need to use a package which is not already installed, please contact the CCB team via the address below before attempting to install a local copy. In many cases the CCB team can easily add it to the central installation.\nAdvanced usage # The setup of the R-cbrg module uses the following system.\nThe R-base module contains fixed, unchanging installations of the base language. This is for safety â€“- they cannot be accidentally overwritten causing unexpected changes of behaviour. The module R-cbrg contain separate package and library repositories for each version of Python. Because packages and library versions also change over time, we take a snapshot of the state on a monthly basis and then lock this to prevent changes causing unexpected behaviour. A single current version for each provides a continual rolling \u0026lsquo;head\u0026rsquo; where changes are applied. Loading the R-cbrg module will automatically pull in the latest stable base and all packages or libraries.\nFor instance:\nmodule load R-cbrg module list    However, if you want to use a different version of the base, you can do that by loading it manually first:\nmodule load R-base/3.6.3 module load R-cbrg module list    Simmilarly, if you want to use a different version of the libraries, for example because a recent update broke something you relied on, you can do that by loading it manually:\nmodule load R-cbrg/202108 module list    Getting help # You can email the CCB team using the email address genmail@molbiol.ox.ac.uk. Using this address ensures your email is logged and assigned a tracking number, and will go to all the core team, which means the appropriate person or people will be able to pick it up.\nCopyright # This text is copyright University of Oxford and MRC and may not be reproduced or redistributed without permission.\nAuthor # Duncan Tooke (duncan.tooke@imm.ox.ac.uk) and Kevin Rue-Albrecht (kevin.rue-albrecht@imm.ox.ac.uk).\n"}).add({id:77,href:"/Help/docs/r/setup/",title:"Setup",description:"R setup doks.",content:""}).add({id:78,href:"/Help/docs/r/setup/windows/",title:"Setup R on Windows",description:"Setting up R on Windows.",content:"Motivation # Instructions on this page should be followed in order to produce the desired effects, namely:\n R must be installed before the RStudio IDE, so that the RStudio IDE detects at least one version of R.  Install R # Download the installer file # In your web browser, navigate to https://cran.r-project.org/bin/windows/base/.\nOn the web page, click on the link \u0026lsquo;Download R-x.x.x for Windows\u0026rsquo; (where x.x.x is the latest version of R available for Windows).\n  This should download an excutable file to your computer.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nWe recommend leaving all options to their default values.\nTest your installation #  Double-click on the desktop icon R x.x.x.  The RGui application should open with an R Console.\n  You can close the application. If prompted, do not save the workspace image.\nInstall RStudio Desktop # Download the installer file # In your web browser, navigate to https://posit.co/download/rstudio-desktop/.\nScroll past \u0026ldquo;Step 1: Install R\u0026rdquo; (which was demonstrated above for Windows).\nOn the web page, click on the button \u0026lsquo;Download RStudio Desktop for Windows\u0026rsquo;.\n  This should download an excutable file to your computer.\nRun the installer # Double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nWe recommend leaving all options to their default values.\nTest your installation # Once the installer has completed successfully:\n Open the Windows Start Menu. In the Windows Start Menu find and launch \u0026lsquo;RStudio\u0026rsquo;.  The RStudio application should open.\n  "}).add({id:79,href:"/Help/docs/r/setup/bioconductor/",title:"Install Bioconductor",description:"Installing Bioconductor.",content:"Motivation # The Bioconductor project provides many R packages focused on the analysis of biological assays.\nInstall BiocManager # Instructions are available on the main Bioconductor website.\nBriefly, first launch RStudio.\nThen, run the following command in the R console:\nif (!require(\u0026quot;BiocManager\u0026quot;, quietly = TRUE)) install.packages(\u0026quot;BiocManager\u0026quot;)  RTools # When running the command above, you may be prompted with the following warning message:\n  In which case, please navigate to https://cran.rstudio.com/bin/windows/Rtools/.\n  Click on the link \u0026lsquo;RTools x.x\u0026rsquo; (where x.x is a version of RTools). In particular, select a version of RTools that matches the version of R that you are currently using.\nðŸ‘‰  The version of R currently active can be displayed in the RStudio console typing 'R.version.string'. For instance:     On the next page, click on the link to download the RTools installer (e.g., this is \u0026lsquo;Rtools42 installer\u0026rsquo; for RTools 4.2).\nThen, double-click on the installer file that you just downloaded and progress through the screens of the installation program.\nWe recommend leaving all options to their default values.\nWe also recommend closing RStudio and launching a new RStudio application.\nInstall the latest version of Bioconductor # In the R console of the RStudio application, run the following command:\nBiocManager::install()  You should see messages similar to the following.\n  In this example, Bioconductor version 3.15 was installed, as the latest version of Bioconductor compatible with R 4.2.2.\nðŸ‘‰  The 'BiocManager' package automatically uses the version of R running in the current session to determine the version of Biocondutor packages that can be installed in the current R library. We will describe the process of updating R and Bioconductor in a separate section.   "}).add({id:80,href:"/Help/docs/pipelines/drmaa/",title:"The DRMAA API",description:"Setting up the DRMAA API.",content:"Motivation # The Distributed Resource Management Application API (DRMAA) â†’ is used by workflow managers such as cgatcore â†’ to communicate with HPC queue managers while submitting and monitoring jobs.\nThe drmaa package provides a Python interface to the DRMAA library.\nThis page describes the setup necessary for using DRMAA on the CCB cluster. This setup is a prerequisite for using cgatcore â†’ pipelines.\nSetup # On the CCB cluster, in your ~/.bashrc file, add the following chunk of code to set the environment variable DRMAA_LIBRARY_PATH.\nðŸ‘‰  If your '.bashrc' file already contains the conditional statement 'if [[ $PS1 ]]', you can add only the 'export' statement within the existing bloc of code.   if [[ $PS1 ]]; then export DRMAA_LIBRARY_PATH=/usr/lib64/libdrmaa.so fi  You are now ready to use the DRMAA library in cgatcore â†’ pipelines.\n"}).add({id:81,href:"/Help/docs/pipelines/cgatcore/",title:"The cgatcore library",description:"Using cgatcore pipelines.",content:"Motivation # CGAT-core is a workflow management system that allows users to build data analysis pipelines.\nThe cgatcore python package provides a set of libraries and helper functions that enable researchers to design and build computational workflows for large-scale data analysis workflows.\nIn-depth documentation about cgatcore is available on ReadTheDocs â†’.\nIn this page, we provide advice for setting up the cgatcore workflow management system on the WIMM CCB cluster.\nSetup # On the CCB cluster, create the file ~/.cgat.yml and add the following lines in it.\ncluster: queue_manager: slurm queue: batch tmpdir: /tmp  First, the section cluster defines the program acting as queue manager and the name of the queue to which jobs will be submitted using the DRMAA library.\nThe field tmpdir defines the temporary directory used by cgatcore for writing temporary files during the execution of workflows.\n"}).add({id:82,href:"/Help/docs/help/contact/",title:"Contact",description:"Contacts for getting help.",content:"Email # For issues with the MRC WIMM CCB HPC cluster, email genmail@molbiol.ox.ac.uk to open a ticket that will be tracked by the system administrators.\nGitHub # For issues with this documentation, open a GitHub issue â†’.\n"}).add({id:83,href:"/Help/docs/help/links/",title:"Links to external resources",description:"Links to external resources for further reading are gathered here.",content:"University of Oxford #  MRC Weatherall Institute of Molecular Medicine MRC WIMM Centre for Computational Biology IT Help  Conda #  Conda documentation website Conda documentation Miniconda documentation  "}).add({id:84,href:"/Help/docs/help/contributing/",title:"Contributing",description:"How to contribute to this documentation.",content:"Contributing # Contribute to this documentation on GitHub â†’.\n"}).add({id:85,href:"/Help/docs/",title:"Docs",description:"Docs Doks.",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()